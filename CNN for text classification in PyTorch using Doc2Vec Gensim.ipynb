{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification for Yelp Restaurant Reviews using CNN in PyTorch\n",
    "- For article [Click Here](https://towardsdatascience.com/sentiment-classification-using-cnn-in-pytorch-fba3c6840430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate Word2Vec model and save it plus KeyedVectors (weights)\n",
    "2. Create input tensor which has the index from Word2Vec model as the representer of each word plus pad token index for empty places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:87% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:87% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the original dataset:\n",
      "\n",
      "Index(['review_id', 'user_id', 'business_id', 'stars', 'date', 'text',\n",
      "       'useful', 'funny', 'cool'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5261668"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data_df = pd.read_csv('yelp_review.csv')\n",
    "print(\"Columns in the original dataset:\\n\")\n",
    "print(top_data_df.columns)\n",
    "len(top_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the data is available, mapping from stars to sentiment is done and distribution for each sentiment is plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows per star rating:\n",
      "5    2253347\n",
      "4    1223316\n",
      "1     731363\n",
      "3     615481\n",
      "2     438161\n",
      "Name: stars, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('dark_background')\n",
    "\n",
    "print(\"Number of rows per star rating:\")\n",
    "print(top_data_df['stars'].value_counts())\n",
    "\n",
    "# Function to map stars to sentiment\n",
    "def map_sentiment(stars_received):\n",
    "    if stars_received <= 2:\n",
    "        return -1\n",
    "    elif stars_received == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "# Mapping stars to sentiment into three categories\n",
    "top_data_df['sentiment'] = [ map_sentiment(x) for x in top_data_df['stars']]\n",
    "# Plotting the sentiment distribution\n",
    "plt.figure()\n",
    "pd.value_counts(top_data_df['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"No. of rows in df\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive : 1\n",
    "### Negative: -1\n",
    "### Neutral: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's create a samller subsample of the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2189500</td>\n",
       "      <td>3ZC0Bn3XvJYGs9FUJpt0og</td>\n",
       "      <td>7gL0foPFp3Nui3K7PTWPtw</td>\n",
       "      <td>2HDL09kYFltw77X2G-Kwyg</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>Kirby and the guys treat you like family. Best...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2170885</td>\n",
       "      <td>KmPbtiESkhnSo6NItOiZig</td>\n",
       "      <td>QwPCdaMMEoxcqhN3DN_4WA</td>\n",
       "      <td>BcW7Z9lPmOB_8eS2lEuOqQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>Too bad I cannot give a negative star because ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2366333</td>\n",
       "      <td>XTPmmCh4DzNgK3P8sDgCxQ</td>\n",
       "      <td>DzyBToexbfik0zt7ppky-Q</td>\n",
       "      <td>HutsUcVbARYK_ye7lGtKhg</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>Since I am always trying to find the perfect D...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2951344</td>\n",
       "      <td>_gld1nMMzXJLllZFwz84-Q</td>\n",
       "      <td>fHR3NigJcqU03HX-z3WOFg</td>\n",
       "      <td>ciMCFExQ2_yoBsdkDSWOlw</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>Excellent doctor! Excellent treatment plan and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>326772</td>\n",
       "      <td>4mYUxEWaLRGeexkDyVs7zg</td>\n",
       "      <td>YBJVuZ4gTMLV3DBjZ16s9A</td>\n",
       "      <td>KSUpHqi0pu-rpouVs-KVdA</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-11-14</td>\n",
       "      <td>Rien de spectaculaire! Service courtois mais s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526162</th>\n",
       "      <td>2782542</td>\n",
       "      <td>gaN4m3GV0c1cjl3Nb77qPQ</td>\n",
       "      <td>F3dKpfp0EpxkL-rDZSKzvA</td>\n",
       "      <td>0v8icS8wOOgEDiHDCOQkZQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-11-17</td>\n",
       "      <td>Great Chili chicken\\n\\nHad the dry with noodle...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526163</th>\n",
       "      <td>5238642</td>\n",
       "      <td>m8_g2ApN2DUDs3_rDtawEQ</td>\n",
       "      <td>33DAuLLsKBlfGw7QsTHK5Q</td>\n",
       "      <td>T7yKffTuhiIZclsgjfgowA</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>Great atmosphere and food. Try the meatball sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526164</th>\n",
       "      <td>1098467</td>\n",
       "      <td>vnhnvNxAT4lKosbQohyZOw</td>\n",
       "      <td>HKPdEatcYoEfOL8MEKsWOQ</td>\n",
       "      <td>QbbpMJiSU4M5g3x-q-lLuQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>Just moved to Charlotte in September '15 and j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526165</th>\n",
       "      <td>791452</td>\n",
       "      <td>cNThdWKI50ah46I58mLISw</td>\n",
       "      <td>mcq1qdkjI7M-E1BXeFXstg</td>\n",
       "      <td>JG9UpsFR6hrqQqsKh_RyaQ</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>Been wanting to try Planta for a long time aft...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526166</th>\n",
       "      <td>3898907</td>\n",
       "      <td>j04tsl7YVWbTZ4dVKFM8pg</td>\n",
       "      <td>eTLv_JIPxrwZECVLSKgG_w</td>\n",
       "      <td>CztDemHy1q1f2R3VV9jmvA</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-10-27</td>\n",
       "      <td>Nice furnishing, but that doesn't necessarily ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526167 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          index               review_id                 user_id  \\\n",
       "0       2189500  3ZC0Bn3XvJYGs9FUJpt0og  7gL0foPFp3Nui3K7PTWPtw   \n",
       "1       2170885  KmPbtiESkhnSo6NItOiZig  QwPCdaMMEoxcqhN3DN_4WA   \n",
       "2       2366333  XTPmmCh4DzNgK3P8sDgCxQ  DzyBToexbfik0zt7ppky-Q   \n",
       "3       2951344  _gld1nMMzXJLllZFwz84-Q  fHR3NigJcqU03HX-z3WOFg   \n",
       "4        326772  4mYUxEWaLRGeexkDyVs7zg  YBJVuZ4gTMLV3DBjZ16s9A   \n",
       "...         ...                     ...                     ...   \n",
       "526162  2782542  gaN4m3GV0c1cjl3Nb77qPQ  F3dKpfp0EpxkL-rDZSKzvA   \n",
       "526163  5238642  m8_g2ApN2DUDs3_rDtawEQ  33DAuLLsKBlfGw7QsTHK5Q   \n",
       "526164  1098467  vnhnvNxAT4lKosbQohyZOw  HKPdEatcYoEfOL8MEKsWOQ   \n",
       "526165   791452  cNThdWKI50ah46I58mLISw  mcq1qdkjI7M-E1BXeFXstg   \n",
       "526166  3898907  j04tsl7YVWbTZ4dVKFM8pg  eTLv_JIPxrwZECVLSKgG_w   \n",
       "\n",
       "                   business_id  stars        date  \\\n",
       "0       2HDL09kYFltw77X2G-Kwyg      5  2016-11-15   \n",
       "1       BcW7Z9lPmOB_8eS2lEuOqQ      1  2016-08-05   \n",
       "2       HutsUcVbARYK_ye7lGtKhg      3  2013-01-09   \n",
       "3       ciMCFExQ2_yoBsdkDSWOlw      5  2017-01-31   \n",
       "4       KSUpHqi0pu-rpouVs-KVdA      3  2014-11-14   \n",
       "...                        ...    ...         ...   \n",
       "526162  0v8icS8wOOgEDiHDCOQkZQ      5  2015-11-17   \n",
       "526163  T7yKffTuhiIZclsgjfgowA      5  2016-01-09   \n",
       "526164  QbbpMJiSU4M5g3x-q-lLuQ      5  2016-03-04   \n",
       "526165  JG9UpsFR6hrqQqsKh_RyaQ      3  2017-03-01   \n",
       "526166  CztDemHy1q1f2R3VV9jmvA      2  2012-10-27   \n",
       "\n",
       "                                                     text  useful  funny  \\\n",
       "0       Kirby and the guys treat you like family. Best...       1      1   \n",
       "1       Too bad I cannot give a negative star because ...       1      2   \n",
       "2       Since I am always trying to find the perfect D...       6      5   \n",
       "3       Excellent doctor! Excellent treatment plan and...       0      0   \n",
       "4       Rien de spectaculaire! Service courtois mais s...       1      0   \n",
       "...                                                   ...     ...    ...   \n",
       "526162  Great Chili chicken\\n\\nHad the dry with noodle...       1      1   \n",
       "526163  Great atmosphere and food. Try the meatball sa...       0      0   \n",
       "526164  Just moved to Charlotte in September '15 and j...       0      0   \n",
       "526165  Been wanting to try Planta for a long time aft...       0      0   \n",
       "526166  Nice furnishing, but that doesn't necessarily ...       1      0   \n",
       "\n",
       "        cool  sentiment  \n",
       "0          1          1  \n",
       "1          0         -1  \n",
       "2          2          0  \n",
       "3          0          1  \n",
       "4          0          0  \n",
       "...      ...        ...  \n",
       "526162     1          1  \n",
       "526163     0          1  \n",
       "526164     0          1  \n",
       "526165     0          0  \n",
       "526166     2         -1  \n",
       "\n",
       "[526167 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data_df_small = pd.DataFrame.sample(top_data_df, frac = 0.1).reset_index()\n",
    "top_data_df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3ZC0Bn3XvJYGs9FUJpt0og</td>\n",
       "      <td>7gL0foPFp3Nui3K7PTWPtw</td>\n",
       "      <td>2HDL09kYFltw77X2G-Kwyg</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>Kirby and the guys treat you like family. Best...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KmPbtiESkhnSo6NItOiZig</td>\n",
       "      <td>QwPCdaMMEoxcqhN3DN_4WA</td>\n",
       "      <td>BcW7Z9lPmOB_8eS2lEuOqQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>Too bad I cannot give a negative star because ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XTPmmCh4DzNgK3P8sDgCxQ</td>\n",
       "      <td>DzyBToexbfik0zt7ppky-Q</td>\n",
       "      <td>HutsUcVbARYK_ye7lGtKhg</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>Since I am always trying to find the perfect D...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_gld1nMMzXJLllZFwz84-Q</td>\n",
       "      <td>fHR3NigJcqU03HX-z3WOFg</td>\n",
       "      <td>ciMCFExQ2_yoBsdkDSWOlw</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>Excellent doctor! Excellent treatment plan and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4mYUxEWaLRGeexkDyVs7zg</td>\n",
       "      <td>YBJVuZ4gTMLV3DBjZ16s9A</td>\n",
       "      <td>KSUpHqi0pu-rpouVs-KVdA</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-11-14</td>\n",
       "      <td>Rien de spectaculaire! Service courtois mais s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  3ZC0Bn3XvJYGs9FUJpt0og  7gL0foPFp3Nui3K7PTWPtw  2HDL09kYFltw77X2G-Kwyg   \n",
       "1  KmPbtiESkhnSo6NItOiZig  QwPCdaMMEoxcqhN3DN_4WA  BcW7Z9lPmOB_8eS2lEuOqQ   \n",
       "2  XTPmmCh4DzNgK3P8sDgCxQ  DzyBToexbfik0zt7ppky-Q  HutsUcVbARYK_ye7lGtKhg   \n",
       "3  _gld1nMMzXJLllZFwz84-Q  fHR3NigJcqU03HX-z3WOFg  ciMCFExQ2_yoBsdkDSWOlw   \n",
       "4  4mYUxEWaLRGeexkDyVs7zg  YBJVuZ4gTMLV3DBjZ16s9A  KSUpHqi0pu-rpouVs-KVdA   \n",
       "\n",
       "   stars        date                                               text  \\\n",
       "0      5  2016-11-15  Kirby and the guys treat you like family. Best...   \n",
       "1      1  2016-08-05  Too bad I cannot give a negative star because ...   \n",
       "2      3  2013-01-09  Since I am always trying to find the perfect D...   \n",
       "3      5  2017-01-31  Excellent doctor! Excellent treatment plan and...   \n",
       "4      3  2014-11-14  Rien de spectaculaire! Service courtois mais s...   \n",
       "\n",
       "   useful  funny  cool  sentiment  \n",
       "0       1      1     1          1  \n",
       "1       1      2     0         -1  \n",
       "2       6      5     2          0  \n",
       "3       0      0     0          1  \n",
       "4       1      0     0          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top_data_df_small.reset_index(inplace = True, drop = True)\n",
    "top_data_df_small.drop(columns = 'index', inplace=True)\n",
    "top_data_df_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [kirby, and, the, guys, treat, you, like, fami...\n",
      "1    [too, bad, cannot, give, negative, star, becau...\n",
      "2    [since, am, always, trying, to, find, the, per...\n",
      "3    [excellent, doctor, excellent, treatment, plan...\n",
      "4    [rien, de, spectaculaire, service, courtois, m...\n",
      "5    [this, place, has, good, sushi, however, the, ...\n",
      "6    [by, far, the, worst, boston, pizza, ever, bas...\n",
      "7    [attended, young, professional, group, event, ...\n",
      "8    [excellent, bbq, been, going, here, for, about...\n",
      "9    [came, back, here, to, bring, my, sister, and,...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['text']] \n",
    "print(top_data_df_small['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3ZC0Bn3XvJYGs9FUJpt0og</td>\n",
       "      <td>7gL0foPFp3Nui3K7PTWPtw</td>\n",
       "      <td>2HDL09kYFltw77X2G-Kwyg</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>Kirby and the guys treat you like family. Best...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[kirby, and, the, guys, treat, you, like, fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KmPbtiESkhnSo6NItOiZig</td>\n",
       "      <td>QwPCdaMMEoxcqhN3DN_4WA</td>\n",
       "      <td>BcW7Z9lPmOB_8eS2lEuOqQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>Too bad I cannot give a negative star because ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[too, bad, cannot, give, negative, star, becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XTPmmCh4DzNgK3P8sDgCxQ</td>\n",
       "      <td>DzyBToexbfik0zt7ppky-Q</td>\n",
       "      <td>HutsUcVbARYK_ye7lGtKhg</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>Since I am always trying to find the perfect D...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[since, am, always, trying, to, find, the, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_gld1nMMzXJLllZFwz84-Q</td>\n",
       "      <td>fHR3NigJcqU03HX-z3WOFg</td>\n",
       "      <td>ciMCFExQ2_yoBsdkDSWOlw</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>Excellent doctor! Excellent treatment plan and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[excellent, doctor, excellent, treatment, plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4mYUxEWaLRGeexkDyVs7zg</td>\n",
       "      <td>YBJVuZ4gTMLV3DBjZ16s9A</td>\n",
       "      <td>KSUpHqi0pu-rpouVs-KVdA</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-11-14</td>\n",
       "      <td>Rien de spectaculaire! Service courtois mais s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[rien, de, spectaculaire, service, courtois, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  3ZC0Bn3XvJYGs9FUJpt0og  7gL0foPFp3Nui3K7PTWPtw  2HDL09kYFltw77X2G-Kwyg   \n",
       "1  KmPbtiESkhnSo6NItOiZig  QwPCdaMMEoxcqhN3DN_4WA  BcW7Z9lPmOB_8eS2lEuOqQ   \n",
       "2  XTPmmCh4DzNgK3P8sDgCxQ  DzyBToexbfik0zt7ppky-Q  HutsUcVbARYK_ye7lGtKhg   \n",
       "3  _gld1nMMzXJLllZFwz84-Q  fHR3NigJcqU03HX-z3WOFg  ciMCFExQ2_yoBsdkDSWOlw   \n",
       "4  4mYUxEWaLRGeexkDyVs7zg  YBJVuZ4gTMLV3DBjZ16s9A  KSUpHqi0pu-rpouVs-KVdA   \n",
       "\n",
       "   stars        date                                               text  \\\n",
       "0      5  2016-11-15  Kirby and the guys treat you like family. Best...   \n",
       "1      1  2016-08-05  Too bad I cannot give a negative star because ...   \n",
       "2      3  2013-01-09  Since I am always trying to find the perfect D...   \n",
       "3      5  2017-01-31  Excellent doctor! Excellent treatment plan and...   \n",
       "4      3  2014-11-14  Rien de spectaculaire! Service courtois mais s...   \n",
       "\n",
       "   useful  funny  cool  sentiment  \\\n",
       "0       1      1     1          1   \n",
       "1       1      2     0         -1   \n",
       "2       6      5     2          0   \n",
       "3       0      0     0          1   \n",
       "4       1      0     0          0   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [kirby, and, the, guys, treat, you, like, fami...  \n",
       "1  [too, bad, cannot, give, negative, star, becau...  \n",
       "2  [since, am, always, trying, to, find, the, per...  \n",
       "3  [excellent, doctor, excellent, treatment, plan...  \n",
       "4  [rien, de, spectaculaire, service, courtois, m...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have a new column: 'tokenized_text'\n",
    "top_data_df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [kirbi, and, the, gui, treat, you, like, famil...\n",
       "1    [too, bad, cannot, give, neg, star, becaus, th...\n",
       "2    [sinc, am, alwai, try, to, find, the, perfect,...\n",
       "3    [excel, doctor, excel, treatment, plan, and, v...\n",
       "4    [rien, de, spectaculair, servic, courtoi, mai,...\n",
       "5    [thi, place, ha, good, sushi, howev, the, serv...\n",
       "6    [by, far, the, worst, boston, pizza, ever, bas...\n",
       "7    [attend, young, profession, group, event, here...\n",
       "8    [excel, bbq, been, go, here, for, about, year,...\n",
       "9    [came, back, here, to, bring, my, sister, and,...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "top_data_df_small['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in top_data_df_small['tokenized_text'] ]\n",
    "top_data_df_small['stemmed_tokens'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3ZC0Bn3XvJYGs9FUJpt0og</td>\n",
       "      <td>7gL0foPFp3Nui3K7PTWPtw</td>\n",
       "      <td>2HDL09kYFltw77X2G-Kwyg</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>Kirby and the guys treat you like family. Best...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[kirby, and, the, guys, treat, you, like, fami...</td>\n",
       "      <td>[kirbi, and, the, gui, treat, you, like, famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KmPbtiESkhnSo6NItOiZig</td>\n",
       "      <td>QwPCdaMMEoxcqhN3DN_4WA</td>\n",
       "      <td>BcW7Z9lPmOB_8eS2lEuOqQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>Too bad I cannot give a negative star because ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[too, bad, cannot, give, negative, star, becau...</td>\n",
       "      <td>[too, bad, cannot, give, neg, star, becaus, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XTPmmCh4DzNgK3P8sDgCxQ</td>\n",
       "      <td>DzyBToexbfik0zt7ppky-Q</td>\n",
       "      <td>HutsUcVbARYK_ye7lGtKhg</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>Since I am always trying to find the perfect D...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[since, am, always, trying, to, find, the, per...</td>\n",
       "      <td>[sinc, am, alwai, try, to, find, the, perfect,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_gld1nMMzXJLllZFwz84-Q</td>\n",
       "      <td>fHR3NigJcqU03HX-z3WOFg</td>\n",
       "      <td>ciMCFExQ2_yoBsdkDSWOlw</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>Excellent doctor! Excellent treatment plan and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[excellent, doctor, excellent, treatment, plan...</td>\n",
       "      <td>[excel, doctor, excel, treatment, plan, and, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4mYUxEWaLRGeexkDyVs7zg</td>\n",
       "      <td>YBJVuZ4gTMLV3DBjZ16s9A</td>\n",
       "      <td>KSUpHqi0pu-rpouVs-KVdA</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-11-14</td>\n",
       "      <td>Rien de spectaculaire! Service courtois mais s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[rien, de, spectaculaire, service, courtois, m...</td>\n",
       "      <td>[rien, de, spectaculair, servic, courtoi, mai,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  3ZC0Bn3XvJYGs9FUJpt0og  7gL0foPFp3Nui3K7PTWPtw  2HDL09kYFltw77X2G-Kwyg   \n",
       "1  KmPbtiESkhnSo6NItOiZig  QwPCdaMMEoxcqhN3DN_4WA  BcW7Z9lPmOB_8eS2lEuOqQ   \n",
       "2  XTPmmCh4DzNgK3P8sDgCxQ  DzyBToexbfik0zt7ppky-Q  HutsUcVbARYK_ye7lGtKhg   \n",
       "3  _gld1nMMzXJLllZFwz84-Q  fHR3NigJcqU03HX-z3WOFg  ciMCFExQ2_yoBsdkDSWOlw   \n",
       "4  4mYUxEWaLRGeexkDyVs7zg  YBJVuZ4gTMLV3DBjZ16s9A  KSUpHqi0pu-rpouVs-KVdA   \n",
       "\n",
       "   stars        date                                               text  \\\n",
       "0      5  2016-11-15  Kirby and the guys treat you like family. Best...   \n",
       "1      1  2016-08-05  Too bad I cannot give a negative star because ...   \n",
       "2      3  2013-01-09  Since I am always trying to find the perfect D...   \n",
       "3      5  2017-01-31  Excellent doctor! Excellent treatment plan and...   \n",
       "4      3  2014-11-14  Rien de spectaculaire! Service courtois mais s...   \n",
       "\n",
       "   useful  funny  cool  sentiment  \\\n",
       "0       1      1     1          1   \n",
       "1       1      2     0         -1   \n",
       "2       6      5     2          0   \n",
       "3       0      0     0          1   \n",
       "4       1      0     0          0   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [kirby, and, the, guys, treat, you, like, fami...   \n",
       "1  [too, bad, cannot, give, negative, star, becau...   \n",
       "2  [since, am, always, trying, to, find, the, per...   \n",
       "3  [excellent, doctor, excellent, treatment, plan...   \n",
       "4  [rien, de, spectaculaire, service, courtois, m...   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [kirbi, and, the, gui, treat, you, like, famil...  \n",
       "1  [too, bad, cannot, give, neg, star, becaus, th...  \n",
       "2  [sinc, am, alwai, try, to, find, the, perfect,...  \n",
       "3  [excel, doctor, excel, treatment, plan, and, v...  \n",
       "4  [rien, de, spectaculair, servic, courtoi, mai,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have a new column: 'stemmed_tokens'\n",
    "top_data_df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data_df_small.to_csv('top_data_df_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(top_data_df_small, stratify = top_data_df_small.stars, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368316, 157851)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set new indices for both dataframes and drop the previus indices\n",
    "train.reset_index(drop=True), test.reset_index(drop = True)\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('yelp_reviews_train.csv', index = False)\n",
    "test.to_csv('yelp_reviews_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m5jjU8KhAPmDSa5BIopIqw</td>\n",
       "      <td>F9vYcUknd9JY2lxsaEObQQ</td>\n",
       "      <td>T6ihfy4SYiF4PvuE6Y0VPA</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-29</td>\n",
       "      <td>Airport Wendy's. You curbed my hunger. That wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[airport, wendy, curbed, hunger, needed, fries...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pCURaqs8o9kCOl6fEVcsKA</td>\n",
       "      <td>H5d_nFqzwrREE-YduK2ABg</td>\n",
       "      <td>fPpO5751xJI78__uTU2q7g</td>\n",
       "      <td>5</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>I stumbled across this store on my way to Nest...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[stumbled, across, store, way, nest, right, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2C8Gr_EX_gVTlJsobcey6w</td>\n",
       "      <td>xycmBfvZtDX9Bao9kwNQCw</td>\n",
       "      <td>sdE4iWulUozJXOxzQ5Bjhw</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>Pizza was decent. Very disappointed in the del...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[pizza, decent, disappointed, delivery, told, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JWwPv1cIS0YfiQrKtcL9nA</td>\n",
       "      <td>dccateTjyakPfsWd5U0wsQ</td>\n",
       "      <td>K6fYrrTorlpXmqutRcrHzg</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>My first time: the bartenders were so cute [an...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[first, time, bartenders, cute, happy, second,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xW3umQlqu00xiu9UgkBDHw</td>\n",
       "      <td>OvpTIjhGpg2y2kklHa47NQ</td>\n",
       "      <td>Jt28TYWanzKrJYYr0Tf1MQ</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-12-11</td>\n",
       "      <td>I was in las vegas staying at the Paris hotel ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[las, vegas, staying, paris, hotel, sisters, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  m5jjU8KhAPmDSa5BIopIqw  F9vYcUknd9JY2lxsaEObQQ  T6ihfy4SYiF4PvuE6Y0VPA   \n",
       "1  pCURaqs8o9kCOl6fEVcsKA  H5d_nFqzwrREE-YduK2ABg  fPpO5751xJI78__uTU2q7g   \n",
       "2  2C8Gr_EX_gVTlJsobcey6w  xycmBfvZtDX9Bao9kwNQCw  sdE4iWulUozJXOxzQ5Bjhw   \n",
       "3  JWwPv1cIS0YfiQrKtcL9nA  dccateTjyakPfsWd5U0wsQ  K6fYrrTorlpXmqutRcrHzg   \n",
       "4  xW3umQlqu00xiu9UgkBDHw  OvpTIjhGpg2y2kklHa47NQ  Jt28TYWanzKrJYYr0Tf1MQ   \n",
       "\n",
       "   stars        date                                               text  \\\n",
       "0      3  2015-01-29  Airport Wendy's. You curbed my hunger. That wa...   \n",
       "1      5  2008-01-13  I stumbled across this store on my way to Nest...   \n",
       "2      3  2016-05-22  Pizza was decent. Very disappointed in the del...   \n",
       "3      3  2010-01-15  My first time: the bartenders were so cute [an...   \n",
       "4      3  2014-12-11  I was in las vegas staying at the Paris hotel ...   \n",
       "\n",
       "   useful  funny  cool  sentiment  \\\n",
       "0       1      2     2          0   \n",
       "1      19      6    13          1   \n",
       "2       0      3     0          0   \n",
       "3       3      1     1          0   \n",
       "4       0      0     2          0   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [airport, wendy, curbed, hunger, needed, fries...  \n",
       "1  [stumbled, across, store, way, nest, right, ne...  \n",
       "2  [pizza, decent, disappointed, delivery, told, ...  \n",
       "3  [first, time, bartenders, cute, happy, second,...  \n",
       "4  [las, vegas, staying, paris, hotel, sisters, b...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top_data_df_small = pd.read_csv('top_data_df_small')\n",
    "top_data_df_small = pd.read_pickle(\"./top_data_df_small.pkl\")\n",
    "top_data_df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52617"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_data_df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./yelp_reviews_train.pkl\")\n",
    "test = pd.read_pickle(\"./yelp_reviews_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36831, 15786)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These layers are used to find patterns by sliding small kernel window over input. Instead of multiplying the filters on the small regions of the images, it slides through embedding vectors of few words as mentioned by window size. \n",
    "- For looking at sequences of word embeddings, the window has to look at multiple word embeddings in a sequence. They will be rectangular with size window_size * embedding_size. For example, in our case if window size is 3 and embedding size is 500, then kernel will be 3*500. This essentially represents n-grams in the model.\n",
    "- The kernel weights (filter) are multiplied to word embeddings in pairs and summed up to get output values. As the network is being learned, these kernel weights are also being learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Conv Filter](https://miro.medium.com/max/626/1*A094Vuq3OiLFVD2ogxUS7Q.gif \"chess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and output channels for Convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We are feeding only one feature i.e. word embedding so the first parameter for conv2d is 1 (like grayscale images) and output_channels is total number of features which will be NUM_FILTERS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxpooing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Once we have the feature vector and it has extracted the significant features, it is enough to know that it exists in sentence like some positive phrase “great food” and it does not matter where it appears in the sentence. \n",
    " \n",
    "- Maxpooling is used to just get that information and discard the rest of it. For example, in the above animation the feature vector we had, after applying maxpooling, the max value will be chosen. In the above case it shows max when very and delicious are in the phrase, which makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating input and output tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We're going to use `Doc2Vec` by gensim\n",
    "- All review will be converted to a tensor of fixed length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sent2vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from scipy import spatial\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "from sent2vec.splitter import Splitter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [airport, wendy, curbed, hunger, needed, fries...\n",
       "1    [stumbled, across, store, way, nest, right, ne...\n",
       "2    [pizza, decent, disappointed, delivery, told, ...\n",
       "3    [first, time, bartenders, cute, happy, second,...\n",
       "4    [las, vegas, staying, paris, hotel, sisters, b...\n",
       "5    [friend, stopped, around, saturday, 5th, waiti...\n",
       "6    [looking, rolex, stolen, spa, sunday, july, ma...\n",
       "7    [awesome, office, friendly, staff, come, see, ...\n",
       "8    [lunch, meeting, took, find, place, still, mor...\n",
       "9    [awesome, one, favorite, stops, work, came, hu...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data_df_small.stemmed_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ducuments in the data is: 52617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['airport', 'wendy', 'curbed', 'hunger', 'needed', 'fries', 'tuuuurible', 'loaded', 'salt', 'pepper', 'dipped', 'away', 'ketchup', 'wendy', 'probably', 'tables', 'get', 'tight', 'place', 'go', 'eat', 'gate', 'tables', 'gate', 'became', 'awkward', 'eating', 'fries', 'bag', 'ground', 'balancing', 'soda', 'legs', 'holding', 'book', 'one', 'hand', 'burger', 'yes', 'would', 'like', 'tables', 'airport', 'problem', 'wendy', 'also', 'reminded', 'always', 'better', 'mcdonald', 'burger', 'king', 'okay', 'book', 'wendy'], tags=[0]),\n",
       " TaggedDocument(words=['stumbled', 'across', 'store', 'way', 'nest', 'right', 'next', 'door', 'love', 'love', 'love', 'furniture', 'expensive', 'high', 'quality', 'traditional', 'store', 'small', 'jammed', 'packed', 'full', 'furniture', 'fine', 'accessories', 'looking', 'classic', 'design', 'floor', 'lamp', 'real', 'impressive', 'designs', 'also', 'liked', 'several', 'wall', 'mirrors', 'well', 'buffet', 'lamps', 'nice', 'artwork', 'also', 'employee', 'professional', 'designers', 'guide', 'designing', 'perfect', 'new', 'room'], tags=[1]),\n",
       " TaggedDocument(words=['pizza', 'decent', 'disappointed', 'delivery', 'told', 'would', 'house', 'minutes', 'came', 'less', 'prepared', 'caught', 'guard', 'mad', 'furious', 'horrible', 'customer', 'service', 'pizza', 'good', 'tho'], tags=[2]),\n",
       " TaggedDocument(words=['first', 'time', 'bartenders', 'cute', 'happy', 'second', 'time', 'bartender', 'seemed', 'bit', 'jaded', 'quiet', 'impatient', 'although', 'knowledgeable', 'indecisive', 'beer', 'selection', 'large', 'selection', 'least', 'drafts', 'bottles', 'styles', 'mostly', 'american', 'microbrews', 'least', 'large', 'bottled', 'ones', 'beer', 'getting', 'prices', 'reasonable', 'walls', 'brick', 'small', 'tvs', 'placed', 'almost', 'every', 'seat', 'decent', 'view', 'beer', 'selection', 'better', 'places', 'tremont', 'times', 'wintery', 'weekday', 'nights', 'place', 'dead', 'back', 'eventually', 'though', 'probably', 'springtime', 'patio', 'open', 'happy', 'hour'], tags=[3]),\n",
       " TaggedDocument(words=['las', 'vegas', 'staying', 'paris', 'hotel', 'sisters', 'bachelorette', 'weekend', 'decided', 'come', 'breakfast', 'walking', 'whole', 'strip', 'thought', 'way', 'early', 'drink', 'didnt', 'get', 'taste', 'drinks', 'girls', 'said', 'strong', 'good', 'recall', 'name', 'breakfast', 'remember', 'finishing', 'whole', 'plate', 'needed', 'fact', 'girls', 'devoured', 'food', 'outdoors', 'seating', 'nice', 'people', 'watching', 'plan', 'next', 'adventure', 'would', 'return', 'future', 'bachelorette', 'weekend'], tags=[4])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#words_list is a list of list (list of all reveiews where each review is convetred into tokens) + 'pad' token as a seperate review\n",
    "words_list = [x for x in top_data_df_small['stemmed_tokens']]\n",
    "docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(words_list)] # tagged Documents\n",
    "print(f\"number of ducuments in the data is: {len(docs)}\")\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.doc2vec.Doc2Vec"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim \n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(docs, vector_size = 300, min_count=1, epoch = 30)\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airport', 'wendy', 'curbed', 'hunger', 'needed', 'fries', 'tuuuurible', 'loaded', 'salt', 'pepper', 'dipped', 'away', 'ketchup', 'wendy', 'probably', 'tables', 'get', 'tight', 'place', 'go', 'eat', 'gate', 'tables', 'gate', 'became', 'awkward', 'eating', 'fries', 'bag', 'ground', 'balancing', 'soda', 'legs', 'holding', 'book', 'one', 'hand', 'burger', 'yes', 'would', 'like', 'tables', 'airport', 'problem', 'wendy', 'also', 'reminded', 'always', 'better', 'mcdonald', 'burger', 'king', 'okay', 'book', 'wendy']\n",
      "\n",
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.70102865e-02, -3.28667648e-02, -7.58062378e-02,  1.86999977e-01,\n",
       "       -5.71061596e-02, -1.41477957e-01, -4.36102338e-02,  5.08234650e-03,\n",
       "       -7.32778981e-02,  3.09163854e-02,  1.22178808e-01,  5.15694544e-02,\n",
       "       -4.01053727e-02, -1.39332920e-01,  1.43719926e-01,  7.37304911e-02,\n",
       "       -5.61312586e-02, -2.10011913e-03, -2.19460186e-02, -5.31443954e-02,\n",
       "       -2.36852542e-01,  7.19503239e-02, -6.75014630e-02, -5.50109968e-02,\n",
       "        7.50751719e-02,  7.23573416e-02,  5.41276596e-02, -4.79805432e-02,\n",
       "        2.23088823e-03, -3.04220300e-02, -1.80115420e-02,  1.11837462e-01,\n",
       "        1.99755386e-01,  4.86937836e-02, -3.46923135e-02, -7.88292810e-02,\n",
       "       -9.83174052e-03,  9.74044576e-03,  7.49345170e-03, -9.11588073e-02,\n",
       "       -1.23039231e-01,  7.27808774e-02,  4.60539088e-02, -4.37726341e-02,\n",
       "       -1.85607076e-02, -7.55400443e-03, -2.00974628e-01, -4.20418270e-02,\n",
       "       -1.12139275e-02,  8.33669119e-03, -6.04116358e-02, -1.22287534e-02,\n",
       "        1.08174138e-01,  9.86159965e-02, -2.40077171e-02,  3.52765545e-02,\n",
       "        4.83477674e-02,  2.10115984e-02,  6.39563799e-02, -7.87292421e-02,\n",
       "        2.82128272e-03,  7.83348680e-02,  1.10897645e-02, -2.50980584e-03,\n",
       "       -7.56757259e-02,  8.94575641e-02,  9.13570151e-02,  2.22700238e-02,\n",
       "       -1.56676024e-02,  5.05049340e-02, -3.40778120e-02, -2.01143697e-01,\n",
       "        7.14184716e-02,  1.44127920e-01, -1.55586228e-01,  1.80522390e-02,\n",
       "       -3.29716280e-02,  8.18263963e-02, -2.36620437e-02, -3.61651964e-02,\n",
       "       -6.51681749e-03, -4.70723212e-02,  9.15442556e-02,  2.40866328e-03,\n",
       "       -4.26768474e-02,  1.28808357e-02, -6.93057803e-03, -4.31649387e-02,\n",
       "       -1.66129485e-01, -1.12923505e-02, -1.05822124e-01,  1.28002390e-01,\n",
       "       -2.41209697e-02, -6.22107740e-03, -6.61539733e-02, -1.33087132e-02,\n",
       "       -6.31082356e-02,  7.23168701e-02, -5.26132919e-02,  8.63326564e-02,\n",
       "        6.84285210e-03,  3.03175934e-02, -2.46762969e-02,  9.21338983e-03,\n",
       "        1.54719641e-02,  8.27821270e-02,  2.63931020e-03, -7.83890560e-02,\n",
       "        9.36062708e-02,  4.18127924e-02, -7.03537166e-02,  1.09974697e-01,\n",
       "        7.95267448e-02,  9.36789438e-02,  2.01842543e-02,  6.96114451e-02,\n",
       "        2.00661104e-02,  7.07113370e-02,  6.39198497e-02,  5.83507679e-02,\n",
       "        1.94766283e-01, -9.49542075e-02,  1.07464809e-02, -4.77473112e-03,\n",
       "       -1.65549177e-03,  3.42909954e-02, -1.09981392e-02, -4.67058755e-02,\n",
       "       -5.21472692e-02,  1.34225693e-02,  4.69515752e-03, -1.73525080e-01,\n",
       "        6.64347038e-02, -3.95913050e-02,  3.20941992e-02,  7.15269744e-02,\n",
       "        1.65240895e-02, -5.88439181e-02, -5.05633876e-02, -1.97392136e-01,\n",
       "       -8.89704674e-02,  1.17752805e-01,  5.92067018e-02,  7.39552081e-02,\n",
       "        3.28401737e-02,  2.03924701e-02, -2.28191763e-02, -1.05375081e-01,\n",
       "       -1.75555766e-01, -9.38605517e-02,  2.67484225e-02, -5.98246492e-02,\n",
       "       -4.40229811e-02, -1.50035797e-02,  1.04560092e-01, -4.72100899e-02,\n",
       "        1.26984581e-01,  5.95344491e-02,  7.15607777e-02,  3.24354460e-03,\n",
       "       -5.92651665e-02,  1.55040264e-01, -5.01046628e-02,  9.35252756e-03,\n",
       "        1.08400851e-01,  1.02288656e-01, -9.78171602e-02, -5.26971817e-02,\n",
       "       -1.97668727e-02,  6.32447749e-03,  1.90201383e-02,  2.14051697e-02,\n",
       "        2.64866203e-02,  1.12838998e-01,  7.84865245e-02, -6.33827299e-02,\n",
       "        1.20003708e-01, -5.53470217e-02,  9.62297022e-02,  8.61833245e-02,\n",
       "        5.84345311e-02,  1.45579338e-01, -5.42428903e-02, -5.07383347e-02,\n",
       "        8.90506878e-02,  1.02191836e-01, -8.28620326e-03,  3.46001441e-04,\n",
       "       -1.39598474e-01, -6.60451576e-02, -1.05647016e-02, -4.33934294e-02,\n",
       "       -9.34894904e-02,  2.39906199e-02,  2.58619711e-02,  3.45910899e-02,\n",
       "       -1.47594986e-02,  1.68166142e-02, -6.15979470e-02,  6.34189555e-03,\n",
       "        5.03347963e-02, -1.90398917e-01, -9.86494776e-03,  5.54696694e-02,\n",
       "       -1.12535544e-02, -1.37547618e-02,  7.20817223e-02,  4.87141730e-03,\n",
       "       -1.39670940e-02, -1.58402860e-01,  6.68824017e-02, -2.04846300e-02,\n",
       "       -2.70589478e-02, -4.78046313e-02, -7.91820697e-03, -1.55176401e-01,\n",
       "       -1.20890975e-01, -1.31572351e-01, -1.11724645e-01, -1.43848956e-01,\n",
       "        2.02792627e-03, -5.43627813e-02, -6.07073084e-02,  1.01945952e-01,\n",
       "        7.57219270e-03, -3.76213081e-02,  3.50387581e-02,  1.67136767e-03,\n",
       "        1.23693170e-02,  1.21118374e-01,  4.93801087e-02,  4.97414954e-02,\n",
       "        8.52965266e-02, -1.49728820e-01,  1.54544385e-02,  1.03145130e-01,\n",
       "       -9.04038399e-02, -6.89816400e-02,  7.74004832e-02,  1.48653463e-02,\n",
       "        2.79595070e-02, -3.41610387e-02, -1.21411271e-02,  2.40922123e-02,\n",
       "        1.15258740e-02, -6.18270822e-02, -2.49837469e-02, -4.46874127e-02,\n",
       "        1.93087962e-02,  6.66817203e-02,  1.90006062e-01, -8.14361218e-03,\n",
       "        5.86729869e-02,  1.72082316e-02, -1.36009432e-04,  7.37269446e-02,\n",
       "       -6.26722872e-02, -3.09099089e-02, -9.42190513e-02, -5.35130315e-02,\n",
       "       -7.76515156e-02, -7.26122856e-02, -2.77748611e-02,  5.38841188e-02,\n",
       "        1.94579214e-01, -2.59248745e-02,  1.82204954e-02, -1.72267228e-01,\n",
       "       -1.18288085e-01,  5.11450320e-02,  5.12398183e-02, -1.12829812e-01,\n",
       "        8.55289549e-02, -3.31981517e-02,  4.49986532e-02, -2.58647464e-02,\n",
       "        1.28357261e-02,  8.49147961e-02,  1.73383078e-03,  7.65986042e-03,\n",
       "       -2.14677136e-02, -2.72670016e-02, -1.96350878e-03, -7.75885731e-02,\n",
       "        1.71764828e-02, -2.88753305e-02, -6.34843782e-02,  5.52007817e-02,\n",
       "       -9.50235128e-02, -2.36507952e-02,  3.23990844e-02, -1.04447303e-03,\n",
       "        1.35965958e-01,  1.49948774e-02,  1.09108835e-01, -1.72483757e-01,\n",
       "        9.14489571e-03,  2.13293545e-02,  4.27292958e-02, -1.51784211e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Doc vector: note here we use the original document tokens, not the tagged version\n",
    "rev = words_list[0]\n",
    "print(rev)\n",
    "print()\n",
    "print(model.infer_vector(rev).shape)\n",
    "model.infer_vector(rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For creating the output tensor, mapping from label to positive values has to be done. \n",
    "- Currently we had -1 for negative, this is not possible in neural network. \n",
    "- Three neurons in the output layer will give probabilities for each label so we just need mapping to positive numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the output tensor\n",
    "def make_target(label):\n",
    "    if label == -1:\n",
    "        return torch.tensor([0], dtype=torch.long, device=device)\n",
    "    elif label == 0:\n",
    "        return torch.tensor([1], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        return torch.tensor([2], dtype=torch.long, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class make_dataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        if type(dataframe) == str: # when input is the name of a csv file\n",
    "            df = pd.read_csv(dataframe)\n",
    "        else: # when a dataframe is directly given\n",
    "            df = dataframe\n",
    "                    \n",
    "        X = df['stemmed_tokens'].apply(model.infer_vector)\n",
    "        self.X = torch.tensor(X, dtype = torch.float32, device=device) # these are decimals\n",
    "        \n",
    "        self.y = df['sentiment'].apply(make_target) # returns 0, 1 or 2 as label \n",
    "        #self.y = torch.tensor(y, dtype = torch.float32) # these are 0 or 1 floats\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):    \n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36831, 15786)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = make_dataset(train)\n",
    "test_data = make_dataset(test)\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0186,  0.0169, -0.0513,  0.0302,  0.0508, -0.0257,  0.0389, -0.0472,\n",
       "        -0.0114,  0.0031,  0.0255, -0.0093, -0.0074, -0.0480,  0.0434, -0.0787,\n",
       "        -0.0195,  0.0429, -0.0399,  0.0239, -0.0276,  0.0547, -0.0262, -0.0795,\n",
       "         0.0265,  0.0285,  0.0206, -0.0630,  0.0356, -0.0010,  0.0004, -0.0140,\n",
       "        -0.0276,  0.0728,  0.0096, -0.0340,  0.0558, -0.0370, -0.0209, -0.0395,\n",
       "         0.0102,  0.0165,  0.0554,  0.0047, -0.0166,  0.0174, -0.0612,  0.0281,\n",
       "         0.0304,  0.0100, -0.0361, -0.0001, -0.0189,  0.0525, -0.0028,  0.0044,\n",
       "        -0.0313,  0.0800, -0.0042, -0.0072,  0.0123,  0.0363,  0.0365,  0.0398,\n",
       "         0.0385,  0.0511, -0.0119,  0.0061,  0.0010,  0.0592, -0.0174, -0.0083,\n",
       "         0.0473,  0.0600, -0.0158, -0.0016,  0.0320,  0.0541, -0.0035, -0.0539,\n",
       "         0.0362, -0.0800, -0.0216,  0.0040,  0.0059, -0.0522, -0.0534,  0.0105,\n",
       "        -0.0293,  0.0113, -0.0528,  0.0490, -0.0040, -0.0415, -0.0959, -0.0348,\n",
       "        -0.0083,  0.0402,  0.0138, -0.0085,  0.0393, -0.0042, -0.0051,  0.0037,\n",
       "         0.0246,  0.0097, -0.0040,  0.0124, -0.0046,  0.0028, -0.0416,  0.0236,\n",
       "        -0.0352,  0.0642,  0.0212, -0.0718, -0.0048,  0.0152,  0.0185, -0.0085,\n",
       "        -0.0111, -0.0084, -0.0042,  0.0261,  0.0408, -0.0290, -0.0359, -0.0200,\n",
       "        -0.0319,  0.0196, -0.0148, -0.0156, -0.0061,  0.0134,  0.0441, -0.0050,\n",
       "         0.0352, -0.0299, -0.0257, -0.0257, -0.0603, -0.0060,  0.0400,  0.0104,\n",
       "         0.0125,  0.0193,  0.0223, -0.0146, -0.0405, -0.0060, -0.0471,  0.0526,\n",
       "        -0.0815, -0.0335, -0.0002,  0.0195,  0.0344,  0.0806,  0.0532,  0.0153,\n",
       "         0.0330,  0.0305, -0.0193,  0.0433, -0.0260, -0.0257,  0.0598, -0.0002,\n",
       "         0.0631,  0.0080, -0.0547, -0.0252, -0.0533,  0.0741, -0.0066, -0.0414,\n",
       "         0.0254, -0.0252, -0.0065, -0.0175,  0.0098,  0.0552,  0.0023, -0.0030,\n",
       "         0.0441, -0.0259,  0.0198, -0.0064, -0.0128,  0.0188,  0.0706, -0.0245,\n",
       "        -0.0235, -0.0033,  0.0166, -0.0261, -0.0594, -0.0401, -0.0181,  0.0556,\n",
       "         0.0251, -0.0105,  0.0452, -0.0276, -0.0556, -0.0066,  0.0348,  0.0148,\n",
       "        -0.0183, -0.0449, -0.0123, -0.0124,  0.0255,  0.0578,  0.0161, -0.0261,\n",
       "        -0.0325, -0.0020,  0.0373, -0.0170, -0.0058, -0.0252, -0.0246, -0.0148,\n",
       "         0.0403, -0.0354,  0.0121, -0.0179, -0.0453, -0.0153, -0.0448, -0.0307,\n",
       "         0.0032, -0.0550, -0.0085, -0.0004, -0.0141, -0.0018,  0.0247, -0.0490,\n",
       "        -0.0039, -0.0438, -0.0296,  0.0446,  0.0605, -0.0260, -0.0462, -0.0096,\n",
       "         0.0157,  0.0820, -0.0078,  0.0087,  0.0088, -0.0533,  0.0050,  0.0530,\n",
       "         0.0320, -0.0133, -0.0166, -0.0024, -0.0101, -0.0124, -0.0172,  0.0203,\n",
       "         0.0539,  0.0064,  0.0531, -0.0749, -0.0494,  0.0223, -0.0146, -0.0259,\n",
       "         0.0691, -0.0299, -0.0358, -0.0110, -0.0375, -0.0419, -0.0242, -0.0209,\n",
       "         0.0274, -0.0074, -0.0308, -0.0325,  0.0271,  0.0320, -0.0212, -0.0057,\n",
       "        -0.0006,  0.0115, -0.0575,  0.0685, -0.0575, -0.0297,  0.0073, -0.0517,\n",
       "         0.0191, -0.0549,  0.0179, -0.0130], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[32][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 32, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 32, drop_last=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "\n",
    "predictors, target = next(data_iter)\n",
    "print(predictors.squeeze(1).shape, target.shape) # each train batch has 2 elements in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 100])\n",
      "Conv1d(1, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(32,100,1) # 32 is batch_size, 100 is dimnesion of input vector, 1 is added to work with conv1d\n",
    "output = nn.Conv1d(in_channels =100,out_channels=1,kernel_size=1,stride=1)(tensor)\n",
    "output.shape \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "a = torch.randn(32, 100, 1) \n",
    "a = a.permute(0, 2, 1) # (32, 1, 100)\n",
    "m = nn.Conv1d(1, 50, 3, padding = 1) \n",
    "out = m(a) \n",
    "print(out.size())# (32, 50, 100)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_SIZE = 500\n",
    "NUM_FILTERS = 100\n",
    "import gensim\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, window_sizes=(1,2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "\n",
    "        # for each window size, 1 conv layer\n",
    "        self.convs = nn.ModuleList([ \n",
    "                               nn.Conv1d(1, NUM_FILTERS, window_size, padding=(window_size - 1))\n",
    "                               for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1) # (32, 1 ,300)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.tanh(conv(x)) #[32, 100, 300] > >[32, 100, 301] >> [32, 100, 302] >> [32, 100, 304]\n",
    "            # print(x2.shape) = torch.squeeze(x2, -1)\n",
    "            \n",
    "            x2 = F.max_pool1d(x2, x2.size(2))  \n",
    "            # print(x2.shape) # [32, 100, 1]\n",
    "            xs.append(x2) # combines all these matricies to from one final matrix of all detected features\n",
    "            # print(\"xs: \", len(xs)) # 4, a list of 4 matricies eaxch is [32, 100, 1]\n",
    "        x = torch.cat(xs, 2) # [32, 100, 4])\n",
    "        \n",
    "        # FC, x.size(0) is the batch_size\n",
    "        x = x.view(x.size(0), -1) # flatten the feature matrix into a vector [32, 400]\n",
    "        #print(x.shape)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### criterion = nn.NLLLoss()\n",
    "NUM_CLASSES = 3\n",
    "#VOCAB_SIZE = len(w2v_model.wv.vocab)\n",
    "\n",
    "cnn_model = CnnTextClassifier( num_classes=NUM_CLASSES)\n",
    "cnn_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "def check_accuracy(data_loader, model):\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        cnn_model.eval()\n",
    "        #for data, targets in test_loader:\n",
    "        for e in range(3):\n",
    "            loop = tqdm(enumerate(data_loader), total = len(data_loader), leave = False)\n",
    "            for batch_idx, (data, targets) in loop:\n",
    "             #X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "                data = data.to(device) \n",
    "                targets = targets.squeeze(1).to(device) # converts [32, 1] to [32]\n",
    "                \n",
    "                predictions = model(data)\n",
    "                val_loss = criterion(predictions, targets)\n",
    "\n",
    "                _, preds = predictions.max(1)\n",
    "                corrects = (preds == targets).float()\n",
    "                test_acc = corrects.sum() / len(corrects) \n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += test_acc.item()\n",
    "            print(f\"\"\"Epoch {e+1}: | Train Loss: {val_epoch_loss/len(data_loader):.5f} | Train Acc: {val_epoch_acc/len(data_loader):.5f}   \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 16/1150 [00:00<00:07, 158.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: | Train Loss: 1.08220 | Train Acc: 0.17092   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/1150 [00:00<00:05, 190.99it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: | Train Loss: 2.16439 | Train Acc: 0.34185   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: | Train Loss: 3.24659 | Train Acc: 0.51277   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-48-1dba38e5a72f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-48-1dba38e5a72f>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    loop = tqdm(enumerate(train_loader), total = len(train_loader). leave = False)\u001b[0m\n\u001b[1;37m                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "    \n",
    "loop = tqdm(enumerate(train_loader), total = len(train_loader). leave = False)\n",
    "for batch_idx, (data, targets) in loop:\n",
    "    \n",
    "    \n",
    "    loop.set_description(epoch/num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "print(\"Begin training.\")\n",
    "EPOCHS = 10\n",
    "for e in trange(1, EPOCHS+1):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    cnn_model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        #print(\"inp: \", inputs.shape, labels.shape)\n",
    "        #inputs = inputs.squeeze(1).to(device) # converts [32, 1, 774] to [32, 774]\n",
    "        labels = labels.squeeze(1).to(device) # converts [32, 1] to [32]\n",
    "        # print(\"inp: \", inputs.shape, labels.shape)\n",
    "        #X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = cnn_model(inputs)\n",
    "        \n",
    "        train_loss = criterion(predictions, labels)\n",
    "        #train_acc = multi_acc(predictions, labels)\n",
    "        #train_acc = check_accuracy(train_loader, cnn_model)\n",
    "        _, preds = predictions.max(1) # _ is the max value, predictions is the max_indx\n",
    "        correct = (preds == labels).float()\n",
    "        #num_samples += preds.size(0)\n",
    "        train_acc = correct.sum() / len(correct) # calculate the accutace for each batch in train_iterator\n",
    "        assert len(correct) ==  32\n",
    "        #round(float(num_correct)/float(num_samples) * 100, 2)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "    \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "\n",
    "        cnn_model.eval()\n",
    "        for data, targets in test_loader:\n",
    "        \n",
    "            targets = targets.squeeze(1).to(device) # converts [32, 1] to [32]\n",
    "            predictions = cnn_model(data)\n",
    "\n",
    "            val_loss = criterion(predictions, targets)\n",
    "\n",
    "            _, preds = predictions.max(1)\n",
    "            corrects = (preds == targets).float()\n",
    "            test_acc = corrects.sum() / len(corrects) \n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += test_acc.item()\n",
    "                              \n",
    "\n",
    "    print(f\"\"\"Epoch {e+0:03}: \n",
    "| Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(test_loader):.5f} \n",
    "| Train Acc: {train_epoch_acc/len(train_loader):.5f}   | Val Acc: {val_epoch_acc/len(test_loader):.5f}\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
