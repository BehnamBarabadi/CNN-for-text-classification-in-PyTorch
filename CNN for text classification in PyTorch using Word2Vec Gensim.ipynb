{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification for Yelp Restaurant Reviews using CNN in PyTorch\n",
    "- For article [Click Here](https://towardsdatascience.com/sentiment-classification-using-cnn-in-pytorch-fba3c6840430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate Word2Vec model and save it plus KeyedVectors (weights)\n",
    "2. Create input tensor which has the index from Word2Vec model as the representer of each word plus pad token index for empty places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the original dataset:\n",
      "\n",
      "Index(['review_id', 'user_id', 'business_id', 'stars', 'date', 'text',\n",
      "       'useful', 'funny', 'cool'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5261668"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data_df = pd.read_csv('yelp_review.csv')\n",
    "print(\"Columns in the original dataset:\\n\")\n",
    "print(top_data_df.columns)\n",
    "len(top_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the data is available, mapping from stars to sentiment is done and distribution for each sentiment is plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows per star rating:\n",
      "5    2253347\n",
      "4    1223316\n",
      "1     731363\n",
      "3     615481\n",
      "2     438161\n",
      "Name: stars, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('dark_background')\n",
    "\n",
    "print(\"Number of rows per star rating:\")\n",
    "print(top_data_df['stars'].value_counts())\n",
    "\n",
    "# Function to map stars to sentiment\n",
    "def map_sentiment(stars_received):\n",
    "    if stars_received <= 2:\n",
    "        return -1\n",
    "    elif stars_received == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "# Mapping stars to sentiment into three categories\n",
    "top_data_df['sentiment'] = [ map_sentiment(x) for x in top_data_df['stars']]\n",
    "# Plotting the sentiment distribution\n",
    "plt.figure()\n",
    "pd.value_counts(top_data_df['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"No. of rows in df\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive : 1\n",
    "### Negative: -1\n",
    "### Neutral: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's create a smaller subsample of the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4528116</td>\n",
       "      <td>m5jjU8KhAPmDSa5BIopIqw</td>\n",
       "      <td>F9vYcUknd9JY2lxsaEObQQ</td>\n",
       "      <td>T6ihfy4SYiF4PvuE6Y0VPA</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-29</td>\n",
       "      <td>Airport Wendy's. You curbed my hunger. That wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3097267</td>\n",
       "      <td>pCURaqs8o9kCOl6fEVcsKA</td>\n",
       "      <td>H5d_nFqzwrREE-YduK2ABg</td>\n",
       "      <td>fPpO5751xJI78__uTU2q7g</td>\n",
       "      <td>5</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>I stumbled across this store on my way to Nest...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2290314</td>\n",
       "      <td>2C8Gr_EX_gVTlJsobcey6w</td>\n",
       "      <td>xycmBfvZtDX9Bao9kwNQCw</td>\n",
       "      <td>sdE4iWulUozJXOxzQ5Bjhw</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>Pizza was decent. Very disappointed in the del...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1146971</td>\n",
       "      <td>JWwPv1cIS0YfiQrKtcL9nA</td>\n",
       "      <td>dccateTjyakPfsWd5U0wsQ</td>\n",
       "      <td>K6fYrrTorlpXmqutRcrHzg</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>My first time: the bartenders were so cute [an...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3184541</td>\n",
       "      <td>xW3umQlqu00xiu9UgkBDHw</td>\n",
       "      <td>OvpTIjhGpg2y2kklHa47NQ</td>\n",
       "      <td>Jt28TYWanzKrJYYr0Tf1MQ</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-12-11</td>\n",
       "      <td>I was in las vegas staying at the Paris hotel ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52612</th>\n",
       "      <td>5076157</td>\n",
       "      <td>b83swwcJgYYUCtuirXnx-A</td>\n",
       "      <td>YWkIeKGcuRFLwxcTBvn6Yg</td>\n",
       "      <td>JD0Wod1xotR3LckHm-Ql8A</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-11-07</td>\n",
       "      <td>I come here all the time for a quick lunch, al...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52613</th>\n",
       "      <td>1011115</td>\n",
       "      <td>xHwTKVMNrwExZ484CSjUOg</td>\n",
       "      <td>J67R37zomRDYB2_TbC6Lnw</td>\n",
       "      <td>Sg9R6OwNBq5Zf-kjiVBxuw</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>Atmosphere is great and the patio is big but t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52614</th>\n",
       "      <td>1677893</td>\n",
       "      <td>nwfHpovi0tXXVuEO6nJbBg</td>\n",
       "      <td>a2MZowCokvZKbFizcVC75g</td>\n",
       "      <td>4foKEzZMx7pL1DWvqLXfcQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>I have been going to Desert Valley Dental for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52615</th>\n",
       "      <td>5041208</td>\n",
       "      <td>hpIVJEHVxUHSVBNSTyA43g</td>\n",
       "      <td>CiXvlCLs-cksW1PcE4aJhw</td>\n",
       "      <td>aqONNC5onqX6EqHHUO1CJA</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>Was looking for a good Italian restaurant and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52616</th>\n",
       "      <td>2500303</td>\n",
       "      <td>8gJIfypbtHbkOmrJ2vnnpg</td>\n",
       "      <td>U3e3Q8cB1nE9MdLCIgfO3g</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>An absolutely fantastic place!\\n\\nMe and three...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52617 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index               review_id                 user_id  \\\n",
       "0      4528116  m5jjU8KhAPmDSa5BIopIqw  F9vYcUknd9JY2lxsaEObQQ   \n",
       "1      3097267  pCURaqs8o9kCOl6fEVcsKA  H5d_nFqzwrREE-YduK2ABg   \n",
       "2      2290314  2C8Gr_EX_gVTlJsobcey6w  xycmBfvZtDX9Bao9kwNQCw   \n",
       "3      1146971  JWwPv1cIS0YfiQrKtcL9nA  dccateTjyakPfsWd5U0wsQ   \n",
       "4      3184541  xW3umQlqu00xiu9UgkBDHw  OvpTIjhGpg2y2kklHa47NQ   \n",
       "...        ...                     ...                     ...   \n",
       "52612  5076157  b83swwcJgYYUCtuirXnx-A  YWkIeKGcuRFLwxcTBvn6Yg   \n",
       "52613  1011115  xHwTKVMNrwExZ484CSjUOg  J67R37zomRDYB2_TbC6Lnw   \n",
       "52614  1677893  nwfHpovi0tXXVuEO6nJbBg  a2MZowCokvZKbFizcVC75g   \n",
       "52615  5041208  hpIVJEHVxUHSVBNSTyA43g  CiXvlCLs-cksW1PcE4aJhw   \n",
       "52616  2500303  8gJIfypbtHbkOmrJ2vnnpg  U3e3Q8cB1nE9MdLCIgfO3g   \n",
       "\n",
       "                  business_id  stars        date  \\\n",
       "0      T6ihfy4SYiF4PvuE6Y0VPA      3  2015-01-29   \n",
       "1      fPpO5751xJI78__uTU2q7g      5  2008-01-13   \n",
       "2      sdE4iWulUozJXOxzQ5Bjhw      3  2016-05-22   \n",
       "3      K6fYrrTorlpXmqutRcrHzg      3  2010-01-15   \n",
       "4      Jt28TYWanzKrJYYr0Tf1MQ      3  2014-12-11   \n",
       "...                       ...    ...         ...   \n",
       "52612  JD0Wod1xotR3LckHm-Ql8A      4  2016-11-07   \n",
       "52613  Sg9R6OwNBq5Zf-kjiVBxuw      2  2014-04-24   \n",
       "52614  4foKEzZMx7pL1DWvqLXfcQ      5  2017-01-09   \n",
       "52615  aqONNC5onqX6EqHHUO1CJA      5  2015-08-26   \n",
       "52616  4JNXUYY8wbaaDmk3BPzlWw      5  2012-01-18   \n",
       "\n",
       "                                                    text  useful  funny  cool  \\\n",
       "0      Airport Wendy's. You curbed my hunger. That wa...       1      2     2   \n",
       "1      I stumbled across this store on my way to Nest...      19      6    13   \n",
       "2      Pizza was decent. Very disappointed in the del...       0      3     0   \n",
       "3      My first time: the bartenders were so cute [an...       3      1     1   \n",
       "4      I was in las vegas staying at the Paris hotel ...       0      0     2   \n",
       "...                                                  ...     ...    ...   ...   \n",
       "52612  I come here all the time for a quick lunch, al...       2      0     0   \n",
       "52613  Atmosphere is great and the patio is big but t...       0      0     0   \n",
       "52614  I have been going to Desert Valley Dental for ...       0      0     0   \n",
       "52615  Was looking for a good Italian restaurant and ...       0      0     0   \n",
       "52616  An absolutely fantastic place!\\n\\nMe and three...       0      1     0   \n",
       "\n",
       "       sentiment  \n",
       "0              0  \n",
       "1              1  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "52612          1  \n",
       "52613         -1  \n",
       "52614          1  \n",
       "52615          1  \n",
       "52616          1  \n",
       "\n",
       "[52617 rows x 11 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data_df_small = pd.DataFrame.sample(top_data_df, frac = 0.01, random_state=42).reset_index()\n",
    "top_data_df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m5jjU8KhAPmDSa5BIopIqw</td>\n",
       "      <td>F9vYcUknd9JY2lxsaEObQQ</td>\n",
       "      <td>T6ihfy4SYiF4PvuE6Y0VPA</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-29</td>\n",
       "      <td>Airport Wendy's. You curbed my hunger. That wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pCURaqs8o9kCOl6fEVcsKA</td>\n",
       "      <td>H5d_nFqzwrREE-YduK2ABg</td>\n",
       "      <td>fPpO5751xJI78__uTU2q7g</td>\n",
       "      <td>5</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>I stumbled across this store on my way to Nest...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2C8Gr_EX_gVTlJsobcey6w</td>\n",
       "      <td>xycmBfvZtDX9Bao9kwNQCw</td>\n",
       "      <td>sdE4iWulUozJXOxzQ5Bjhw</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>Pizza was decent. Very disappointed in the del...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JWwPv1cIS0YfiQrKtcL9nA</td>\n",
       "      <td>dccateTjyakPfsWd5U0wsQ</td>\n",
       "      <td>K6fYrrTorlpXmqutRcrHzg</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>My first time: the bartenders were so cute [an...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xW3umQlqu00xiu9UgkBDHw</td>\n",
       "      <td>OvpTIjhGpg2y2kklHa47NQ</td>\n",
       "      <td>Jt28TYWanzKrJYYr0Tf1MQ</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-12-11</td>\n",
       "      <td>I was in las vegas staying at the Paris hotel ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52612</th>\n",
       "      <td>b83swwcJgYYUCtuirXnx-A</td>\n",
       "      <td>YWkIeKGcuRFLwxcTBvn6Yg</td>\n",
       "      <td>JD0Wod1xotR3LckHm-Ql8A</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-11-07</td>\n",
       "      <td>I come here all the time for a quick lunch, al...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52613</th>\n",
       "      <td>xHwTKVMNrwExZ484CSjUOg</td>\n",
       "      <td>J67R37zomRDYB2_TbC6Lnw</td>\n",
       "      <td>Sg9R6OwNBq5Zf-kjiVBxuw</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>Atmosphere is great and the patio is big but t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52614</th>\n",
       "      <td>nwfHpovi0tXXVuEO6nJbBg</td>\n",
       "      <td>a2MZowCokvZKbFizcVC75g</td>\n",
       "      <td>4foKEzZMx7pL1DWvqLXfcQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>I have been going to Desert Valley Dental for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52615</th>\n",
       "      <td>hpIVJEHVxUHSVBNSTyA43g</td>\n",
       "      <td>CiXvlCLs-cksW1PcE4aJhw</td>\n",
       "      <td>aqONNC5onqX6EqHHUO1CJA</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>Was looking for a good Italian restaurant and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52616</th>\n",
       "      <td>8gJIfypbtHbkOmrJ2vnnpg</td>\n",
       "      <td>U3e3Q8cB1nE9MdLCIgfO3g</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>An absolutely fantastic place!\\n\\nMe and three...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52617 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id                 user_id             business_id  \\\n",
       "0      m5jjU8KhAPmDSa5BIopIqw  F9vYcUknd9JY2lxsaEObQQ  T6ihfy4SYiF4PvuE6Y0VPA   \n",
       "1      pCURaqs8o9kCOl6fEVcsKA  H5d_nFqzwrREE-YduK2ABg  fPpO5751xJI78__uTU2q7g   \n",
       "2      2C8Gr_EX_gVTlJsobcey6w  xycmBfvZtDX9Bao9kwNQCw  sdE4iWulUozJXOxzQ5Bjhw   \n",
       "3      JWwPv1cIS0YfiQrKtcL9nA  dccateTjyakPfsWd5U0wsQ  K6fYrrTorlpXmqutRcrHzg   \n",
       "4      xW3umQlqu00xiu9UgkBDHw  OvpTIjhGpg2y2kklHa47NQ  Jt28TYWanzKrJYYr0Tf1MQ   \n",
       "...                       ...                     ...                     ...   \n",
       "52612  b83swwcJgYYUCtuirXnx-A  YWkIeKGcuRFLwxcTBvn6Yg  JD0Wod1xotR3LckHm-Ql8A   \n",
       "52613  xHwTKVMNrwExZ484CSjUOg  J67R37zomRDYB2_TbC6Lnw  Sg9R6OwNBq5Zf-kjiVBxuw   \n",
       "52614  nwfHpovi0tXXVuEO6nJbBg  a2MZowCokvZKbFizcVC75g  4foKEzZMx7pL1DWvqLXfcQ   \n",
       "52615  hpIVJEHVxUHSVBNSTyA43g  CiXvlCLs-cksW1PcE4aJhw  aqONNC5onqX6EqHHUO1CJA   \n",
       "52616  8gJIfypbtHbkOmrJ2vnnpg  U3e3Q8cB1nE9MdLCIgfO3g  4JNXUYY8wbaaDmk3BPzlWw   \n",
       "\n",
       "       stars        date                                               text  \\\n",
       "0          3  2015-01-29  Airport Wendy's. You curbed my hunger. That wa...   \n",
       "1          5  2008-01-13  I stumbled across this store on my way to Nest...   \n",
       "2          3  2016-05-22  Pizza was decent. Very disappointed in the del...   \n",
       "3          3  2010-01-15  My first time: the bartenders were so cute [an...   \n",
       "4          3  2014-12-11  I was in las vegas staying at the Paris hotel ...   \n",
       "...      ...         ...                                                ...   \n",
       "52612      4  2016-11-07  I come here all the time for a quick lunch, al...   \n",
       "52613      2  2014-04-24  Atmosphere is great and the patio is big but t...   \n",
       "52614      5  2017-01-09  I have been going to Desert Valley Dental for ...   \n",
       "52615      5  2015-08-26  Was looking for a good Italian restaurant and ...   \n",
       "52616      5  2012-01-18  An absolutely fantastic place!\\n\\nMe and three...   \n",
       "\n",
       "       useful  funny  cool  sentiment  \n",
       "0           1      2     2          0  \n",
       "1          19      6    13          1  \n",
       "2           0      3     0          0  \n",
       "3           3      1     1          0  \n",
       "4           0      0     2          0  \n",
       "...       ...    ...   ...        ...  \n",
       "52612       2      0     0          1  \n",
       "52613       0      0     0         -1  \n",
       "52614       0      0     0          1  \n",
       "52615       0      0     0          1  \n",
       "52616       0      1     0          1  \n",
       "\n",
       "[52617 rows x 10 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top_data_df_small.reset_index(inplace = True, drop = True)\n",
    "top_data_df_small.drop(columns = 'index', inplace=True)\n",
    "top_data_df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "- Using Texthero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data_df_small['stemmed_tokens'] = hero.clean(top_data_df_small.text)\n",
    "top_data_df_small['stemmed_tokens'] = hero.tokenize(top_data_df_small.stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(top_data_df_small['stemmed_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m5jjU8KhAPmDSa5BIopIqw</td>\n",
       "      <td>F9vYcUknd9JY2lxsaEObQQ</td>\n",
       "      <td>T6ihfy4SYiF4PvuE6Y0VPA</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-29</td>\n",
       "      <td>Airport Wendy's. You curbed my hunger. That wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[airport, wendy, curbed, hunger, needed, fries...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pCURaqs8o9kCOl6fEVcsKA</td>\n",
       "      <td>H5d_nFqzwrREE-YduK2ABg</td>\n",
       "      <td>fPpO5751xJI78__uTU2q7g</td>\n",
       "      <td>5</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>I stumbled across this store on my way to Nest...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[stumbled, across, store, way, nest, right, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2C8Gr_EX_gVTlJsobcey6w</td>\n",
       "      <td>xycmBfvZtDX9Bao9kwNQCw</td>\n",
       "      <td>sdE4iWulUozJXOxzQ5Bjhw</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>Pizza was decent. Very disappointed in the del...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[pizza, decent, disappointed, delivery, told, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JWwPv1cIS0YfiQrKtcL9nA</td>\n",
       "      <td>dccateTjyakPfsWd5U0wsQ</td>\n",
       "      <td>K6fYrrTorlpXmqutRcrHzg</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>My first time: the bartenders were so cute [an...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[first, time, bartenders, cute, happy, second,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xW3umQlqu00xiu9UgkBDHw</td>\n",
       "      <td>OvpTIjhGpg2y2kklHa47NQ</td>\n",
       "      <td>Jt28TYWanzKrJYYr0Tf1MQ</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-12-11</td>\n",
       "      <td>I was in las vegas staying at the Paris hotel ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[las, vegas, staying, paris, hotel, sisters, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52612</th>\n",
       "      <td>b83swwcJgYYUCtuirXnx-A</td>\n",
       "      <td>YWkIeKGcuRFLwxcTBvn6Yg</td>\n",
       "      <td>JD0Wod1xotR3LckHm-Ql8A</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-11-07</td>\n",
       "      <td>I come here all the time for a quick lunch, al...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[come, time, quick, lunch, meat, halal, fan, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52613</th>\n",
       "      <td>xHwTKVMNrwExZ484CSjUOg</td>\n",
       "      <td>J67R37zomRDYB2_TbC6Lnw</td>\n",
       "      <td>Sg9R6OwNBq5Zf-kjiVBxuw</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>Atmosphere is great and the patio is big but t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[atmosphere, great, patio, big, service, terri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52614</th>\n",
       "      <td>nwfHpovi0tXXVuEO6nJbBg</td>\n",
       "      <td>a2MZowCokvZKbFizcVC75g</td>\n",
       "      <td>4foKEzZMx7pL1DWvqLXfcQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>I have been going to Desert Valley Dental for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[going, desert, valley, dental, years, love, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52615</th>\n",
       "      <td>hpIVJEHVxUHSVBNSTyA43g</td>\n",
       "      <td>CiXvlCLs-cksW1PcE4aJhw</td>\n",
       "      <td>aqONNC5onqX6EqHHUO1CJA</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>Was looking for a good Italian restaurant and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[looking, good, italian, restaurant, definitel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52616</th>\n",
       "      <td>8gJIfypbtHbkOmrJ2vnnpg</td>\n",
       "      <td>U3e3Q8cB1nE9MdLCIgfO3g</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>An absolutely fantastic place!\\n\\nMe and three...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[absolutely, fantastic, place, three, friends,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52617 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id                 user_id             business_id  \\\n",
       "0      m5jjU8KhAPmDSa5BIopIqw  F9vYcUknd9JY2lxsaEObQQ  T6ihfy4SYiF4PvuE6Y0VPA   \n",
       "1      pCURaqs8o9kCOl6fEVcsKA  H5d_nFqzwrREE-YduK2ABg  fPpO5751xJI78__uTU2q7g   \n",
       "2      2C8Gr_EX_gVTlJsobcey6w  xycmBfvZtDX9Bao9kwNQCw  sdE4iWulUozJXOxzQ5Bjhw   \n",
       "3      JWwPv1cIS0YfiQrKtcL9nA  dccateTjyakPfsWd5U0wsQ  K6fYrrTorlpXmqutRcrHzg   \n",
       "4      xW3umQlqu00xiu9UgkBDHw  OvpTIjhGpg2y2kklHa47NQ  Jt28TYWanzKrJYYr0Tf1MQ   \n",
       "...                       ...                     ...                     ...   \n",
       "52612  b83swwcJgYYUCtuirXnx-A  YWkIeKGcuRFLwxcTBvn6Yg  JD0Wod1xotR3LckHm-Ql8A   \n",
       "52613  xHwTKVMNrwExZ484CSjUOg  J67R37zomRDYB2_TbC6Lnw  Sg9R6OwNBq5Zf-kjiVBxuw   \n",
       "52614  nwfHpovi0tXXVuEO6nJbBg  a2MZowCokvZKbFizcVC75g  4foKEzZMx7pL1DWvqLXfcQ   \n",
       "52615  hpIVJEHVxUHSVBNSTyA43g  CiXvlCLs-cksW1PcE4aJhw  aqONNC5onqX6EqHHUO1CJA   \n",
       "52616  8gJIfypbtHbkOmrJ2vnnpg  U3e3Q8cB1nE9MdLCIgfO3g  4JNXUYY8wbaaDmk3BPzlWw   \n",
       "\n",
       "       stars        date                                               text  \\\n",
       "0          3  2015-01-29  Airport Wendy's. You curbed my hunger. That wa...   \n",
       "1          5  2008-01-13  I stumbled across this store on my way to Nest...   \n",
       "2          3  2016-05-22  Pizza was decent. Very disappointed in the del...   \n",
       "3          3  2010-01-15  My first time: the bartenders were so cute [an...   \n",
       "4          3  2014-12-11  I was in las vegas staying at the Paris hotel ...   \n",
       "...      ...         ...                                                ...   \n",
       "52612      4  2016-11-07  I come here all the time for a quick lunch, al...   \n",
       "52613      2  2014-04-24  Atmosphere is great and the patio is big but t...   \n",
       "52614      5  2017-01-09  I have been going to Desert Valley Dental for ...   \n",
       "52615      5  2015-08-26  Was looking for a good Italian restaurant and ...   \n",
       "52616      5  2012-01-18  An absolutely fantastic place!\\n\\nMe and three...   \n",
       "\n",
       "       useful  funny  cool  sentiment  \\\n",
       "0           1      2     2          0   \n",
       "1          19      6    13          1   \n",
       "2           0      3     0          0   \n",
       "3           3      1     1          0   \n",
       "4           0      0     2          0   \n",
       "...       ...    ...   ...        ...   \n",
       "52612       2      0     0          1   \n",
       "52613       0      0     0         -1   \n",
       "52614       0      0     0          1   \n",
       "52615       0      0     0          1   \n",
       "52616       0      1     0          1   \n",
       "\n",
       "                                          stemmed_tokens  \n",
       "0      [airport, wendy, curbed, hunger, needed, fries...  \n",
       "1      [stumbled, across, store, way, nest, right, ne...  \n",
       "2      [pizza, decent, disappointed, delivery, told, ...  \n",
       "3      [first, time, bartenders, cute, happy, second,...  \n",
       "4      [las, vegas, staying, paris, hotel, sisters, b...  \n",
       "...                                                  ...  \n",
       "52612  [come, time, quick, lunch, meat, halal, fan, d...  \n",
       "52613  [atmosphere, great, patio, big, service, terri...  \n",
       "52614  [going, desert, valley, dental, years, love, g...  \n",
       "52615  [looking, good, italian, restaurant, definitel...  \n",
       "52616  [absolutely, fantastic, place, three, friends,...  \n",
       "\n",
       "[52617 rows x 11 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data_df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(top_data_df_small, stratify = top_data_df_small.stars, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36831, 15786)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set new indices for both dataframes and drop the previus indices\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop = True)\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52617"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_data_df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test['stemmed_tokens'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data_df_small.to_pickle(\"./top_data_df_small.pkl\")\n",
    "train.to_pickle(\"./yelp_reviews_train.pkl\")\n",
    "test.to_pickle(\"./yelp_reviews_test.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m5jjU8KhAPmDSa5BIopIqw</td>\n",
       "      <td>F9vYcUknd9JY2lxsaEObQQ</td>\n",
       "      <td>T6ihfy4SYiF4PvuE6Y0VPA</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-29</td>\n",
       "      <td>Airport Wendy's. You curbed my hunger. That wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[airport, wendy, curbed, hunger, needed, fries...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pCURaqs8o9kCOl6fEVcsKA</td>\n",
       "      <td>H5d_nFqzwrREE-YduK2ABg</td>\n",
       "      <td>fPpO5751xJI78__uTU2q7g</td>\n",
       "      <td>5</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>I stumbled across this store on my way to Nest...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[stumbled, across, store, way, nest, right, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2C8Gr_EX_gVTlJsobcey6w</td>\n",
       "      <td>xycmBfvZtDX9Bao9kwNQCw</td>\n",
       "      <td>sdE4iWulUozJXOxzQ5Bjhw</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>Pizza was decent. Very disappointed in the del...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[pizza, decent, disappointed, delivery, told, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JWwPv1cIS0YfiQrKtcL9nA</td>\n",
       "      <td>dccateTjyakPfsWd5U0wsQ</td>\n",
       "      <td>K6fYrrTorlpXmqutRcrHzg</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>My first time: the bartenders were so cute [an...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[first, time, bartenders, cute, happy, second,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xW3umQlqu00xiu9UgkBDHw</td>\n",
       "      <td>OvpTIjhGpg2y2kklHa47NQ</td>\n",
       "      <td>Jt28TYWanzKrJYYr0Tf1MQ</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-12-11</td>\n",
       "      <td>I was in las vegas staying at the Paris hotel ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[las, vegas, staying, paris, hotel, sisters, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  m5jjU8KhAPmDSa5BIopIqw  F9vYcUknd9JY2lxsaEObQQ  T6ihfy4SYiF4PvuE6Y0VPA   \n",
       "1  pCURaqs8o9kCOl6fEVcsKA  H5d_nFqzwrREE-YduK2ABg  fPpO5751xJI78__uTU2q7g   \n",
       "2  2C8Gr_EX_gVTlJsobcey6w  xycmBfvZtDX9Bao9kwNQCw  sdE4iWulUozJXOxzQ5Bjhw   \n",
       "3  JWwPv1cIS0YfiQrKtcL9nA  dccateTjyakPfsWd5U0wsQ  K6fYrrTorlpXmqutRcrHzg   \n",
       "4  xW3umQlqu00xiu9UgkBDHw  OvpTIjhGpg2y2kklHa47NQ  Jt28TYWanzKrJYYr0Tf1MQ   \n",
       "\n",
       "   stars        date                                               text  \\\n",
       "0      3  2015-01-29  Airport Wendy's. You curbed my hunger. That wa...   \n",
       "1      5  2008-01-13  I stumbled across this store on my way to Nest...   \n",
       "2      3  2016-05-22  Pizza was decent. Very disappointed in the del...   \n",
       "3      3  2010-01-15  My first time: the bartenders were so cute [an...   \n",
       "4      3  2014-12-11  I was in las vegas staying at the Paris hotel ...   \n",
       "\n",
       "   useful  funny  cool  sentiment  \\\n",
       "0       1      2     2          0   \n",
       "1      19      6    13          1   \n",
       "2       0      3     0          0   \n",
       "3       3      1     1          0   \n",
       "4       0      0     2          0   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [airport, wendy, curbed, hunger, needed, fries...  \n",
       "1  [stumbled, across, store, way, nest, right, ne...  \n",
       "2  [pizza, decent, disappointed, delivery, told, ...  \n",
       "3  [first, time, bartenders, cute, happy, second,...  \n",
       "4  [las, vegas, staying, paris, hotel, sisters, b...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top_data_df_small = pd.read_csv('top_data_df_small')\n",
    "top_data_df_small = pd.read_pickle(\"./top_data_df_small.pkl\")\n",
    "top_data_df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52617"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_data_df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./yelp_reviews_train.pkl\")\n",
    "test = pd.read_pickle(\"./yelp_reviews_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36831, 15786)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These layers are used to find patterns by sliding small kernel window over input. Instead of multiplying the filters on the small regions of the images, it slides through embedding vectors of few words as mentioned by window size. \n",
    "- For looking at sequences of word embeddings, the window has to look at multiple word embeddings in a sequence. They will be rectangular with size window_size * embedding_size. For example, in our case if window size is 3 and embedding size is 500, then kernel will be 3*500. This essentially represents n-grams in the model.\n",
    "- The kernel weights (filter) are multiplied to word embeddings in pairs and summed up to get output values. As the network is being learned, these kernel weights are also being learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Conv Filter](https://miro.medium.com/max/626/1*A094Vuq3OiLFVD2ogxUS7Q.gif \"chess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and output channels for Convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We are feeding only one feature i.e. word embedding so the first parameter for conv2d is 1 (like grayscale images) and output_channels is total number of features which will be NUM_FILTERS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxpooing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Once we have the feature vector and it has extracted the significant features, it is enough to know that it exists in sentence like some positive phrase “great food” and it does not matter where it appears in the sentence. \n",
    " \n",
    "- Maxpooling is used to just get that information and discard the rest of it. For example, in the above animation the feature vector we had, after applying maxpooling, the max value will be chosen. In the above case it shows max when very and delicious are in the phrase, which makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating input and output tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input will be Word2Vec vectors trained with embedding size 500. As we want to keep the length of sentences the same, padding token will be used to fill extra remaining words when the size of sentence is less than the highest length sentence in the corpus.\n",
    "- Let’s train the Word2Vec model by using following function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!chmod 755 models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sent2vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from scipy import spatial\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "from sent2vec.splitter import Splitter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of list (list of all reveiews where each review is convetred into tokens) + 'pad' token as a seperate review\n",
    "words_list = [x for x in top_data_df_small['stemmed_tokens']]\n",
    "words_list.append('[pad]')\n",
    "words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "size = 500\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1 # 1 for SkipGram otherwise, CBOW\n",
    "\n",
    "w2v_model = Word2Vec(words_list, min_count = 1, size = 500, workers = 3, window = 3, sg = 1)\n",
    "word2vec_file =  'models/' + 'word2vec_' + str(size) + '_PAD.model'\n",
    "w2v_model.save(word2vec_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67996"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number fo words in w2vmodel\n",
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31676582,  0.07629906,  0.12840565, -0.26190862,  0.08043399,\n",
       "        0.07768615, -0.06731065, -0.09520342,  0.2874018 , -0.12392927,\n",
       "       -0.30881032,  0.03571766, -0.1280366 , -0.17926586, -0.04265871,\n",
       "       -0.04278198, -0.12346992,  0.0512777 , -0.00735853, -0.20282559,\n",
       "       -0.0893134 ,  0.11177756, -0.02960602, -0.16376258,  0.07934507,\n",
       "        0.01524296,  0.11616605,  0.10360915, -0.20276146,  0.18994613,\n",
       "        0.02354078,  0.2692116 , -0.00881538,  0.06750507,  0.17672347,\n",
       "        0.21561511, -0.07358584, -0.04221582,  0.06700817, -0.29893652,\n",
       "       -0.09397522, -0.0261348 , -0.12915015,  0.22960377,  0.05646389,\n",
       "        0.28817514,  0.20030904,  0.44291428, -0.06394979,  0.0939202 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['cake'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pizzas', 0.7784745097160339),\n",
       " ('crust', 0.7089414000511169),\n",
       " ('pepperoni', 0.6924998164176941),\n",
       " ('margherita', 0.6862131357192993),\n",
       " ('calzone', 0.6766183376312256)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('pizza',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1053"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index of word 'pad' in w2v_model\n",
    "w2v_model.wv.vocab['pad'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the model is ready, we can create a function to generate input tensor.\n",
    "- The below function creates a tensor of length 888 for each review where each word is replaced by the index of that word from Word2Vec model.\n",
    "- These indices then would be converted to vectora of 500 elements to be given to the Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next line apply len() to each rows of top_data_df_small.stemmed_tokens and find the max of these values\n",
    "max_sen_len = top_data_df_small.stemmed_tokens.map(len).max() # 774 \n",
    "padding_idx = w2v_model.wv.vocab['pad'].index # index of the pad token is 1053\n",
    "\n",
    "def make_word2vec_vector_cnn(sentence):\n",
    "    padded_X = [padding_idx for i in range(max_sen_len)] # first create a list of all pad tokens\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2v_model.wv.vocab:\n",
    "            padded_X[i] = 0\n",
    "            #print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2v_model.wv.vocab[word].index\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774 1053\n"
     ]
    }
   ],
   "source": [
    "print(max_sen_len, padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "and\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  434,  411,   76,    0,  123,  601, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053,\n",
       "         1053, 1053, 1053, 1053, 1053, 1053]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tensor representation of a review\n",
    "# 'I' & 'and' are not in the word2vec vocabulary so they are represented with 0\n",
    "# 'ate' has the key = 434 in the vicabulary\n",
    "# 1053 is the index of pad token to fill the empty space, makes all reviews of the same length\n",
    "make_word2vec_vector_cnn(['I', 'ate', 'cake', 'pizza', 'and', 'hot', 'cafe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52617,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = top_data_df_small['stemmed_tokens'].apply(make_word2vec_vector_cnn)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 774])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention!\n",
    "- Each review is an input to the CNN model\n",
    "- All inputs should be of the same length so we create a tensor of lengthe equal to the longest review\n",
    "- Then each is review converted to one of those tensors. For each word in a review, we insert the index of that word generated by Word2Vec vocab and fill the empty places with the index of pad (words which are not in that specific review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For creating the output tensor, mapping from label to positive values has to be done. \n",
    "- Currently we had -1 for negative, this is not possible in neural network. \n",
    "- Three neurons in the output layer will give probabilities for each label so we just need mapping to positive numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the output tensor\n",
    "def make_target(label):\n",
    "    if label == -1: # negative\n",
    "        return torch.tensor([0], dtype=torch.long, device=device)\n",
    "    elif label == 0: # neural\n",
    "        return torch.tensor([1], dtype=torch.long, device=device)\n",
    "    else: # positive\n",
    "        return torch.tensor([2], dtype=torch.long, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class make_dataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        if type(dataframe) == str: # when input is the name of a csv file\n",
    "            df = pd.read_csv(dataframe)\n",
    "        else: # when a dataframe is directly given\n",
    "            df = dataframe\n",
    "            \n",
    "        #X = df['stemmed_tokens'].values\n",
    "        #y = df.Class.values\n",
    "         \n",
    "        self.X = df['stemmed_tokens'].apply(make_word2vec_vector_cnn)\n",
    "        #print(X.shape)\n",
    "        #self.X = torch.tensor(X, dtype = torch.float32) # these are decimals\n",
    "        \n",
    "        self.y = df['sentiment'].apply(make_target) # returns 0, 1 or 2 as label \n",
    "        #self.y = torch.tensor(y, dtype = torch.float32) # these are 0 or 1 floats\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):    \n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_dataset(train)\n",
    "test_data = make_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36831, 15786)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  251,    95,   601,  3594,    86,  3016,   294,  5008,  2586,    60,\n",
       "             1,  3682,   601,  3594,   478,   651,   294,   457,     1,   193,\n",
       "           363,  1367,  1484, 10965,  1991,   866,     4,  1896,   560,  1367,\n",
       "          4100,    11,  5186,  1009,    27,   349,   141, 12502, 47293,    11,\n",
       "             0,  1490,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,  1053,\n",
       "          1053,  1053,  1053,  1053]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[17][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[32][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 32, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 32, drop_last=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 774]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "\n",
    "predictors, target = next(data_iter)\n",
    "print(predictors.squeeze(1).shape, target.shape) # each train batch has 32 elements in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 774]) tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "\n",
    "predictors, target = next(data_iter)\n",
    "print(predictors.squeeze(1).shape, target) # each train batch has 2 elements in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/inputs.jpg\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/cnn for text.jpg\" width = 700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 500\n",
    "NUM_FILTERS = 10\n",
    "import gensim\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        w2v_model = gensim.models.KeyedVectors.load('models/' + 'word2vec_500_PAD.model') # load the saved model\n",
    "        weights = w2v_model.wv # get the KeydVectors (keyvectors have keys as index of each word)\n",
    "        \n",
    "        # With pretrained embeddings\n",
    "        tensor_weights = torch.FloatTensor(weights.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(tensor_weights, padding_idx=w2v_model.wv.vocab['pad'].index)\n",
    "\n",
    "        # for each window size, 1 conv layer\n",
    "        self.convs = nn.ModuleList([ \n",
    "                               nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n",
    "                               for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"model: \",x.shape) = [32, 774]\n",
    "        # x represents one row of dataframe. It is a vector of 888 elements where each element is an index of a word \n",
    "        x = self.embedding(x) # [Batch, Sequence_length, Embedding] = [32, 774, 500])\n",
    "        #print(\"x after embedding: \", x.shape)\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1) #  [32, 1, 774, 500]) like a grayscale image\n",
    "        xs = []\n",
    "        for conv in self.convs: # we have 4 conv layers with different winodow sizes\n",
    "            x2 = torch.tanh(conv(x))\n",
    "            #print(\"x2 after conv: \", x2.shape) # [32, 10, 774, 1]>>[32, 10, 775, 1]>>[32, 10, 776, 1]>>[32, 10, 778, 1]\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            #print(\"x2 after squeeze: \", x2.shape)# [32, 10, 774] >> [32, 10, 775] >> [32, 10, 776] >> [32, 10, 778]\n",
    "            x2 = F.max_pool1d(x2, x2.size(2)) # keyps only the highets value of each feature vector (detected features for that window size)\n",
    "            #print(\"x2 after maxpool: \", x2.shape) # [32, 10, 1]\n",
    "            xs.append(x2) # combines all these matricies to from one final matrix of all detected features\n",
    "            #print(\"xs: \", len(xs)) # 4, a list of 4 matricies eaxch is [32, 10, 1]\n",
    "        x = torch.cat(xs, 2) # concatanate all matricies in xs on the last dimension(1) to form the final feature matrix\n",
    "        #print(\"x before flatten: \", x.shape) # [32, 10, 4])\n",
    "        # FC, x.size(0) is the batch_size\n",
    "        x = x.view(x.size(0), -1) # flatten the feature matrix into a vector\n",
    "        #print(\"x after flatten: \", x.shape) # [32, 40]\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### criterion = nn.NLLLoss()\n",
    "NUM_CLASSES = 3\n",
    "VOCAB_SIZE = len(w2v_model.wv.vocab)\n",
    "\n",
    "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "cnn_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def check_accuracy(data_loader, model):\n",
    "    model.to(device)\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.squeeze(1).to(device) # converts froom [32, 1, 774] to [32, 774]\n",
    "            y = y.squeeze(1).to(device)\n",
    "            #x = x.reshape(s.shape[0], -1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            print(scores)\n",
    "            _, predictions = scores.max(1) # _ is the max value, predictions is the max_indx\n",
    "            # predictions = torch.round(scores) # for BINARY classification\n",
    "            print(predictions,_, y)\n",
    "            if predictions == y:\n",
    "                print(predictions, y)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            print(num_correct, num_samples)\n",
    "                        \n",
    "    return  round(float(num_correct)/float(num_samples) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2001, -0.4812,  3.9140],\n",
      "        [-1.0135,  0.6460,  0.7117],\n",
      "        [-1.8760, -0.6591,  2.7160],\n",
      "        [ 1.0975,  0.1144,  0.1381],\n",
      "        [-2.5012, -0.0941,  3.0032],\n",
      "        [-0.3712,  0.2576,  1.4193],\n",
      "        [-2.4190, -0.8888,  3.4356],\n",
      "        [-2.4882, -0.5533,  3.7098],\n",
      "        [-1.0818, -1.0533,  2.5664],\n",
      "        [-0.2050,  0.0483,  0.4834],\n",
      "        [-2.6715, -0.2223,  3.3766],\n",
      "        [-1.6949,  0.3704,  1.0109],\n",
      "        [-1.3840,  0.1668,  1.6035],\n",
      "        [ 0.8042, -0.0207, -0.1656],\n",
      "        [ 2.3033, -0.0304, -1.0542],\n",
      "        [ 0.1378,  0.7062, -0.7309],\n",
      "        [-1.7849, -0.4068,  2.3639],\n",
      "        [ 1.4658,  0.1157, -0.4083],\n",
      "        [ 2.9122,  0.3163, -1.2804],\n",
      "        [ 2.1535,  0.8506, -1.8336],\n",
      "        [ 3.1021, -0.5603, -1.0137],\n",
      "        [-1.0753,  0.2473,  0.9598],\n",
      "        [ 0.7472,  0.2129, -0.1648],\n",
      "        [ 0.0480, -0.3185,  0.7240],\n",
      "        [-2.8538, -0.4893,  3.6820],\n",
      "        [ 0.2912,  0.0207,  0.3264],\n",
      "        [ 2.8496, -0.0970, -1.3806],\n",
      "        [-1.0171, -0.1046,  1.7630],\n",
      "        [-1.3619,  0.3425,  0.9798],\n",
      "        [-1.2309,  0.6926,  1.3103],\n",
      "        [-2.1831,  0.4468,  2.0915],\n",
      "        [-0.8136, -0.4580,  2.4137]], device='cuda:0')\n",
      "tensor([2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 2,\n",
      "        2, 2, 0, 2, 2, 2, 2, 2], device='cuda:0') tensor([3.9140, 0.7117, 2.7160, 1.0975, 3.0032, 1.4193, 3.4356, 3.7098, 2.5664,\n",
      "        0.4834, 3.3766, 1.0109, 1.6035, 0.8042, 2.3033, 0.7062, 2.3639, 1.4658,\n",
      "        2.9122, 2.1535, 3.1021, 0.9598, 0.7472, 0.7240, 3.6820, 0.3264, 2.8496,\n",
      "        1.7630, 0.9798, 1.3103, 2.0915, 2.4137], device='cuda:0') tensor([2, 1, 2, 0, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2,\n",
      "        2, 1, 0, 1, 2, 2, 2, 2], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-c492d35ad478>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-e2d0152d9db4>\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[1;34m(data_loader, model)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# predictions = torch.round(scores) # for BINARY classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mnum_correct\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "print(\"Begin training.\")\n",
    "EPOCHS = 10\n",
    "for e in trange(1, EPOCHS+1):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    cnn_model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        #print(\"inp: \", inputs.shape, labels.shape)\n",
    "        inputs = inputs.squeeze(1).to(device) # converts [32, 1, 774] to [32, 774]\n",
    "        labels = labels.squeeze(1).to(device) # converts [32, 1] to [32]\n",
    "        #print(\"inp: \", inputs.shape, labels.shape)\n",
    "        #X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = cnn_model(inputs)\n",
    "        \n",
    "        train_loss = criterion(predictions, labels)\n",
    "        #train_acc = multi_acc(predictions, labels)\n",
    "        #train_acc = check_accuracy(train_loader, cnn_model)\n",
    "        _, preds = predictions.max(1) # _ is the max value, predictions is the max_indx\n",
    "        correct = (preds == labels).float()\n",
    "        #num_samples += preds.size(0)\n",
    "        train_acc = correct.sum() / len(correct) # calculate the accutace for each batch in train_iterator\n",
    "        assert len(correct) ==  32\n",
    "        #round(float(num_correct)/float(num_samples) * 100, 2)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "    \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "\n",
    "        cnn_model.eval()\n",
    "        for data, targets in test_loader:\n",
    "         #X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            data = data.squeeze(1).to(device) # converts froom [32, 1, 774] to [32, 774]\n",
    "            targets = targets.squeeze(1).to(device)\n",
    "            predictions = cnn_model(data)\n",
    "\n",
    "            val_loss = criterion(predictions, targets)\n",
    "\n",
    "            _, preds = predictions.max(1)\n",
    "            corrects = (preds == targets).float()\n",
    "            test_acc = corrects.sum() / len(corrects) \n",
    "            #test_acc = round(float(num_correct)/float(num_samples) * 100, 2)\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += test_acc.item()\n",
    "                              \n",
    "\n",
    "    print(f\"\"\"Epoch {e+0:03}: \n",
    "| Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(test_loader):.5f} \n",
    "| Train Acc: {train_epoch_acc/len(train_loader):.5f}   | Val Acc: {val_epoch_acc/len(test_loader):.5f}\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnTextClassifier(\n",
      "  (embedding): Embedding(67996, 500, padding_idx=1053)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 10, kernel_size=[1, 500], stride=(1, 1))\n",
      "    (1): Conv2d(1, 10, kernel_size=[2, 500], stride=(1, 1), padding=(1, 0))\n",
      "    (2): Conv2d(1, 10, kernel_size=[3, 500], stride=(1, 1), padding=(2, 0))\n",
      "    (3): Conv2d(1, 10, kernel_size=[5, 500], stride=(1, 1), padding=(4, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=40, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.save(cnn_model.state_dict(), \"cnn_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnTextClassifier(\n",
      "  (embedding): Embedding(67996, 500, padding_idx=1053)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 10, kernel_size=[1, 500], stride=(1, 1))\n",
      "    (1): Conv2d(1, 10, kernel_size=[2, 500], stride=(1, 1), padding=(1, 0))\n",
      "    (2): Conv2d(1, 10, kernel_size=[3, 500], stride=(1, 1), padding=(2, 0))\n",
      "    (3): Conv2d(1, 10, kernel_size=[5, 500], stride=(1, 1), padding=(4, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=40, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.KeyedVectors.load('models/' + 'word2vec_500_PAD.model') \n",
    "VOCAB_SIZE = len(w2v_model.wv.vocab)\n",
    "NUM_CLASSES = 3\n",
    "cnn_model = CnnTextClassifier(VOCAB_SIZE, NUM_CLASSES)\n",
    "\n",
    "cnn_model.load_state_dict(torch.load('cnn_model.pkl'))\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "import texthero as hero\n",
    "\n",
    "def test_review(rev):\n",
    "    rev = pd.Series(rev)\n",
    "    rev = hero.clean(rev)\n",
    "    rev = hero.tokenize(rev)\n",
    "    rev = rev.to_list()[0]\n",
    "    rev = make_word2vec_vector_cnn(rev)\n",
    "    _, pred = cnn_model(rev).max(1)\n",
    "    print('negative') if pred.item() == 0 else print('neural') if pred.item() == 1 else print('positive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "Dice is filthy. Crude. Rude. Sexist.\n",
      " Downright nasty. But there is some twinkles of genius in his material, delivery, & stage persona. Grew up during his heyday but was never a fan. I am now. My wife and I laughed a lot. His takes on the nuances of sex were hilarious. His interaction with audience upfront was a pro at work.\n",
      "\n",
      "Eleanor, who opens for him is a piece of work. Also 10 times cruder than Dice. But very talented.\n",
      "\n",
      "We paid $37 per tick via travelzoo. With that said, I'd say Dice Clay was the greatest entertainment value I've ever enjoy in Vegas. Not for most.. None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "negative\n",
      "The place is nice BUT. We arrived on Thursday afternoon and fount dirty towels, gum wrappers, and coke bottles laying out side the doors of the suites next to us. They were still there when we left on Sunday. On Friday I bought a beer at the bar in the lobby and was walking into the pool area with it. You would have thought it was a bomb. The life guard\\/ security guy freaked out and I had to finish it before entering the pool area. Then I could buy the same beer at the pool bar. He also added that if I could read there was a sign by the door. I said I bought it from a Hilton bar at the same hotel what's the difference?  He said only drinks or food at the pool can come from that bar only. It would have been ok if I could have gotten a waitress to ever come over even when they would tell me they would be right back. If you wanted a drink you would have to go to the pool bar where one bartender was working and wait 49 mins in line. To bad the place is nice. None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "negative\n",
      "Went in last night around 5pm , I started looking & noticed they had the sale 7\\/$28 panties so I decided to choose some . So I'm at the vs side and got some of them and I headed to the PINK side to look at those also . I was approached by an employee and she told me they were 7\\/$28 which I knew and she noticed I had some vs brand and she said I could mix & match all the panties in the store so I did . I go to get ringed up and at the register the girl said I couldn't mix & match and some panties were not in the 7\\/$28 so I was like ok I'll go change them and I told her an employee told me i could.So she calls manager and the manager says *loudly* acting like it was my fault , even though an employee told me saying she was gonna let me get away with it and that at this store they don't do that . Acting like it was MY fault and seeming like I was trying to steal  . Everyone in the line could hear that and It was so embarrassing . I left everything and just walked out . This is horrible customer service I will never return to this location again . Maybe it was because the color of my skin I hate to bring the race issue in but it's true ! None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "positive\n",
      "This place has a great feel once you first walk in their door. The fish tank is really cool too! The fish seem exotic or something? Really, really neat. The place is not very big in size, which gives it that intimate dive restaurant feel. The courtyard\\/ patio in the front would be great for a group if you looking for a little more room.\n",
      "\n",
      "This place is right in the heart of old town Glendale. Parking was not an issue at all when I went for lunch time. I parked in the thrift store parking then walked over. Seemed like a few others were doing the same thing as I noticed pulling up to the restaurant. \n",
      "\n",
      "Once I got sat I noticed the menu was mostly in Vietnamese and had English translation as well. I wasn't sure what to order, as it was my very first time being here, or even to a Vietnamese restaurant for that matter. The server recommended the Pho, and a eves dropping customer also insisted that I try the Pho. \n",
      "\n",
      "So I ordered the Beef and Shrimp Pho, along with some fresh spring rolls and  a Coors Light to drink. The Spring rolls where very different than what I was used to. They were extremely fresh wrapped in a weird film like texture of some sort. It was very delicious! The dipping sauce that comes with the rolls is perfect for the rolls. They bring out all the add in for the pho before the bowl of broth arrives. The add in consist of mint, cilantro, lemon, jalapenos, and some type of sprout root. Once I tried the Pho, I was blown away by out great it all was! It was actually really exciting and fun eating Pho since it was my first time. I can now see why everyone keeps recommending the Pho here!\n",
      "\n",
      "What a great experience! I cannot wait to return here again. None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "negative\n",
      "I was here exclusively for desserts last night. Can't agree with other yelpers here, this place did not float my boat.\n",
      "\n",
      "The place wasn't very busy, yet i had to wait 5 minutes before somebody even came and got us seated. They didn't give us any water either.\n",
      "\n",
      "The server took our order later on and i told her specifically to bring the tea with desserts, guess what? she still brought the tea before it and i asked \"I specified to bring the tea with desserts, she is like yaaah the dessert is coming\", well it came after we were halfway through our tea.\n",
      "\n",
      "I would have let all of that go, had the food been good. But sadly it wasn't.\n",
      "\n",
      "I got the surprise me coz i liked the concept of crepes wrapped like a pouch, but the crepe wasn't good. It was sweet, while we all know crepes are supposed to be neuteral. I got the chocolate dip on the side and that was extremely sweet too, i planned on drinking it but i couldn't get meself too.\n",
      "\n",
      "The only thing good here was the ambiance, it looks very inviting, but imo, they have nothing else going on for them for me to come back. None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "positive\n",
      "It was the 3 of us after a day of golf and hiking. we were hungry and we wanted wings. We checked yelp and this place was close and highly rated. So we pulled up and took a seat in the well lit dining room after we ordered up 36: 4 flavors, 9 per flavor. TWC dry rub, original, bbq and honey sriracha. Celery, carrots and blue cheese came as sides. We added regular and sweet potato fries as well as fountain sodas to the order. It took about 5 mins for the order to arrive.\n",
      "\n",
      "We were all very happy with the quality and taste of the meal. The wings were large and juicy as promised. The portions on the fries was ok, and could have been slightly larger. but they're good, so next time I'll get two and try the seasoned variety. the one thing we would have liked more is to have been able to try more flavors, but we're like that. The beignets with caramel dipping sauce at the end was a nice treat. We had a pretty nice time. If you're into wings, try this place. None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "positive\n",
      "We got tickets through House Seats. If it weren't for House Seats, I probably never would have heard of or seen this show.\n",
      "\n",
      "We weren't quite sure what to expect. I knew it was some kind of martial arts demonstration, but that's about all I knew.\n",
      "\n",
      "As it turned out, the show was more of a comedic dance show with a strong martial arts influence. \n",
      "\n",
      "My favorite was a skit with a meditation instructor and an unruly student. The student kept interrupting the instructor and mini-fights ensued. It was pretty amusing.\n",
      "\n",
      "My wife's favorite was the Karate Kid sequence. There is a funny twist to the story line that plays well against the song \"Glory of Love.\"\n",
      "\n",
      "We both enjoyed ourselves. While it's not the highest caliber show on the strip, it was a good time. It's a family friendly show and one I would recommend to families visiting on a budget. None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "positive\n",
      "This is my favorite of all the campus bars hands down. \n",
      "\n",
      "Every Thursday night during the school year, a club I'm in takes over the basement (pool tables, darts and all) to hang out in. I can't even count all the great memories I've made there. \n",
      "\n",
      "Unfortunately, on Friday and Saturday nights the lines build up pretty fast, so it's best to get in before it gets packed.\n",
      "\n",
      "In terms of food\\/drink, great fries, great specials on drinks. ($2 Woodchuck Thursdays!) None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "positive\n",
      "I loved staying here. I just wish my friend and I spent more time in the hotel room or at least got to take advantage of one of the many pools.  But instead w got distracted by all the blackjack tables, and we gambled away late into the night and came back to the room only to sleep and shower.  The next morning I'd be crying about how hungry I was, so we'd leave in a hurry, only never to return until my friend won all the money he lost back (and let me tell you, it took a damn long time for that to happen!) None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "negative\n",
      "Horrible delivery experience with this place. Don't have a car so I wanted delivery. These people left a menu on my door so I thought I'd try it. So I went on what they call their website, which is not very well put together. But what do I care? I'm just hungry. I make my order at 10:45, and I get a call after 12 about how the driver couldn't my apartment and he went back. So they remake my order so they say and deliver it at 1:30. When I got the food, it wasn't fresh at all as I expected, even though when I called back they said it would be fresh when it got here. Never ordering from there again. Horrible. The fact that they put their menu on my door just frustrates me. None\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for x in range(10):\n",
    "    n = random.randint(1,10000)\n",
    "    print (top_data_df_small.text[n], test_review(top_data_df_small.text[n]))\n",
    "    print('----------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
